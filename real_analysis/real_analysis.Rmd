---
title: "Real Analysis"
bibliography: ref.bib
output:
  html_document:
    toc: yes
    toc_depth: 4
    df_print: paged
    self_contained: yes
    css: style.css
    toc_float:
      collapsed: false
      smooth_scroll: false
  pdf_document:
    toc: yes
    keep_tex: yes
    latex_engine: xelatex
  beamer_presentation:
    keep_tex: yes
    toc: yes
    slide_level: 2
    latex_engine: xelatex
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
- \usepackage{amsthm}
- \newtheorem{Definition}{Definition}
- \newtheorem{Theorem}{Theorem}
- \newtheorem{Example}{Example}
---


# Proposistion (Lecture 1)

## Outline

Our focus in this lecture will be on these 3 questions.

1. What is a proposition?
2. What is a logical connective?
3. How logical connectives interact?

------------

## Definition (Discussion)

A proposition (in this course at least) is always assumed to be unambiguous and definite, so that any proposition is always either true or false.
We also assume propositions are objective, so that their truth value is independent from whether they being true or not. 

-------

### Examples

1. London is the capital of the UK.
2. Berlin ist die deutsche Hauptstadt nicht.
3. $3 > 2$.
4. $12345678^{87654321}$ is a prime number.
5. For every real number, there is an integer such that the previous is smaller the latter.

----------

### Nonexamples

1. Real analysis is easy.
2. $x > 2$.
3. Is there a number whose square is 2?

----------

## Connectives

### Negation
Let $A$ be any proposition, we denote $\neg A$ to be the negation of $A$. For example, if $A$ is the proposition "$3 > 2$", then $\neg A$ is "$3 \le 2$".

From previous discussion, we can see:

|$A$ | $\neg A$|
|:--:|:-------:|
| T  |  F      |
|F | T |

----------

### Conjunction

Conjunction is just a fancy way to say "and". For any two propositions $A$ and $B$, we will denote their conjunction to be $A\land B$. $A \land B$ is t true whenever $A$ and $B$ are simultaneously true. For example "$3 > 2 \land 2 > 1$" asserts that three is bigger than two and two is bigger than one.

|$A$|$B$|$A \land B$|
|:-:|:-:|:---------:|
|T|T|T|
|T|F|F|
|F|T|F|
|F|F|F|

Conjunction in some older references might be known as the _logical product_.

----------

### Disjunction

For ant two propositions $A$ and $B$ , we denote their disjunction to be $A\lor B$. $A\lor B$ is true whenever at least one of $A$ $B$ is true. This may be different from everyday "or" which sometimes has an exclusive flavour.

|$A$|$B$|$A \lor B$|
|:-:|:-:|:---------:|
|T|T|T|
|T|F|T|
|F|T|T|
|F|F|F|

Disjunction in some older references might be known as the _logical sum_.

--------------

### Implication
Implication loosely translates to "if ... then ...". Let us denote $A$ implying $B$ by $A \implies B$ or $A \to B$, whichever you prefer. We call in this case $A$ to be antecedent or premise and $B$ to be consequent or conclusion.

|$A$|$B$|$A \implies B$|
|:-:|:-:|:---------:|
|T|T|T|
|T|F|F|
|F|T|T|
|F|F|T|

The last two rows are the baffling statement that _ex falso quodlibet_ which might be worthy of a philosophical debate[^1]. But we will content ourselves with the following discussion.


------

#### For all $x$, $x > 2 \implies x^2>4$

We hope this to be true (intuitionistically). Let us work out explicitly what this says for different $x$.

| $x$ | $x>2$ | $x^2>4$ | $x>2\implies x^2>4$|
|:---:|:----:| :--------:| :-------:|
|3|T|T|T|
|-3|F|T|T|
|0|F|F|T|

So I mean, why not?

This is why $\implies$ only *loosely* translates to "if ... then ..." because the latter contains a causal relation. But for our usage, "Snow is black $\implies$ grass is red" is a *true* statement[^2].

------------

### Equivalence

For propositions $A$ and $B$, $A$ and $B$ are equivalent, denoted by $A \iff B$, is to say A is true whenever B is true and vice versa.

|$A$|$B$|$A \iff B$|
|:-:|:-:|:---------:|
|T|T|T|
|T|F|F|
|F|T|F|
|F|F|T|

-------

#### Exercises


1. Fill in the blank and convince yourself that we are defining equivalence to be $(A\implies B) \land (B \implies A)$

|$A$|$B$|$A\implies B$|$B\implies A$|$A \iff B$|
|:-:|:-:|:-:|:-:|:---------:|
|T|T| | |T|
|T|F| | |F|
|F|T| | |F|
|F|F| | |T|

2. Convince yourself $A \iff \neg\neg A$. This is called the _double negation law_.

3. Convince yourself $(A\implies B)\iff(\neg A\lor B)$

---------

## Quantifiers

### $\forall$
It is somewhat cumbersome to always write statements like "P is true for every x". Instead we write $\forall xP$ or $\forall x, P$ if adding a comma makes the meaning clearer.

### $\exists$
We also denote statements like "there is some x to make P true" as $\exists x P$ or $\exists x, P$.

We also have
$$\neg\forall x P\iff \exists x\neg P$$

-----------

### Exercises
Assume all we know is natural number, what does this mean and is is true:

1. $\forall N,\exists p, P(p)\land (p>N)$ where $P(p)$ is to say $p$ is prime.
2. $\forall m\forall n,m+n>m$
3. $\exists z\exists o\forall m,m+z=m \land m\times o=m$
4. come up with more examples until you are comfortable with $\forall$ and $\exists$.


---------

## Laws of propositions and connectives

Let $A$, $B$, $C$ be propositions

### Commutative laws
1. of conjunction
\[A \land B \iff B\land A\]
2. of disjunction
\[A \lor B \iff B\lor A\]

### Associative laws
1. of conjunction
\[[(A\land B)\land C] \iff [A\land(B\land C)]\]
2. of disjunction
\[[(A\lor B)\lor C] \iff [A\lor(B\lor C)]\]

-----------

Let $A$, $B$, $C$ be propositions

### Distributive laws
1. $[A\land (B\lor C)] \iff [(A\land B)\lor(A\land C)]$
2. $[A\lor (B\land C)] \iff [(A\lor B)\land(A\lor C)]$

### Idempotent laws
1. of conjunction
\[(A\land A)\iff A\]
2. of disjunction
\[(A\lor A)\iff A\]

### Transitive laws
1. of implication
  \[[(A\implies B)\land (B\implies C)]\implies (A\implies C)\]
2. of equivalence
  \[[(A\iff B)\land (B\iff C)]\implies(A\iff C)\]
  
----

Let $A$, $B$ be propositions

### De Morgan's Laws
1. $\neg(A\lor B)\iff(\neg A\land\neg B)$
2. $\neg(A\land B)\iff(\neg A\lor\neg B)$

### Law of contraposition
$(A\implies B)\iff(\neg B\implies \neg A)$

### Law of syllogism
1. $(A\lor\neg A)$ is a tautology.
2. $(A\land\neg A)$ is a contradiction.

### Absorption Laws
Let $T$ be any *true* proposition and $F$ be any *false* proposition

1. $(A\land T)\iff A$
2. $(A\lor T)\iff T$
3. $(A\land F)\iff F$
4. $(A\lor F)\iff A$

--------

## Exercises

Let $A$ and $B$ be propositions, express the following using connectives. Try to see some equivalent representations using the "laws".

1. one of and only one of $A$, $B$ is true;
2. $A$ and $B$ are never true at the same time;
3. at least one of $A$, $B$ is true;
4. create your own until you feel comfortable with connectives.

Let $A$ be "something only about $x$" so that $\forall xA$ and $\exists xA$ are propositions, express the following: 

1. there is a unique x to make $A$ true. (From now on, we denote this as $\exists!x,A$)
2. If you know group theory, try to express the fact that $\mathbb{Z}$ is an additive group.

--------------

## Summary

Please summarise this lecture by answering:

1. So what is a proposition?
2. So what is a logical connective?
3. So how do logical connectives interact?

For more attentive reader, you might find these useful with a deeper view towards first order logic:

1. @hamilton1988logic chapter 1 and 2;
2. @van2004logic chapter 1


# Basic naïve set theory (Lecture 2)


## Disclaimer

In this section we sometimes need to purposely avoid the most rigorous approach. Because otherwise we should develop first order logic and axiomatic set theory which are really quite advanced. In particular, we will not really define what a set is, instead we just pretend that this is an intrinsically clear definition and ignore set-theoretic paradoxes such as Russell.

## What to expect in this lecture

1. roughly what a set is;
2. operations on sets;
3. properties of sets and their operations;
4. relations and functions

----------------

## "Definition" 

For the purpose of this course only, a set is something we can talk about "membership".

So for $A$ any set, if we want to say "$x$ is a member of $A$", we write conveniently $x\in A$; if we want to say "$x$ is not a member of $A$", we write "$x\not\in A$", short for "$\neg(x\in A)$".

* We can write down a set simply by listing all the elements (if this is possible). For example, $\{1,2\}$ is the set with only two elements, namely 1 and 2.
* We can write down a set by its defining property: $\{x | P\}$ where P is some proposition. Then any element in this set satisfies $P$, and everything satisfying $P$ is in the set. For example $\{x|x\in\mathbb N\land x \mathrm{\ even}\}$ is the set $\{0,2,4,6,8,10,...\}$.

For $A$ any set and $p$ some well-defined property, let us agree to short $\forall x,x\in A\implies x \textrm{ has property p}$ as $\forall x\in A$ and $\exists x,x\in A\land x\textrm{ has property p}$ to $\exists x\in A, x\textrm{ has property p}$.

Little exercise: check $\neg(\forall x\in A, x\textrm{ has property p})\iff\exists x\in A,\neg(x \textrm{ has property p})$.

### Axiom of extensionality

We agree that all about sets are elements and nothing else: if $A$ and $B$ are sets then $A=B\iff \forall x, (x\in A\iff x\in B)$. This is the *axiom of extensionality*.

For any set $A$, we write $|A|$ or $\mathrm {card} A$ to be the cardinality of $A$ when $A$ is finite. So, $|{1,2,3}|=3$.

-----------------

### Prehaps the most important set of all. (Exercise)

1. There is a trivial example of set, namely, the set containing nothing.
Prove using the axiom of extensionality that there is only one empty set. Hence we denote this unique empty set as $\emptyset$. If you want to know why I claim such a boring set to be vitally important, consult perhaps @halmos2017naive, in some (weak) sense maths is really about empty set.

2. Please explain with the axiom of extensionality, why $\emptyset$ and $\{\emptyset\}$ are two different object.

In the notion above, $\emptyset$ is the only set with size zero.
If you did the exercise in the last lecture: $$\exists!\emptyset,\forall x,x\notin\emptyset$$

---------------

## Subset and power set

Let $A$ be any set.

*  We say a set $B$ is a subset of $A$ if and only if $\forall x,x\in B\implies x\in A$. In this case, we denote it as $B\subseteq A$. Note that in particular $A\subseteq A$ and $\emptyset\subseteq A$. We also short $B\subseteq A\land B\neq A$ to $B\subsetneq A$ and in this case $B$ is a proper subset of $A$. By $\{x\in A|P\}$, we mean $\{x|x\in A\land P\}$ -- the subset of $A$ whose elements satisfy P. 

* By power set of $A$, written as $P(A)$ or $\mathcal P (A)$, we mean the set of all subsets of $A$.

### Exercise
Prove by ax. of ext., $A=B\iff A\subseteq B\land B\subseteq A$

--------------

Say $A=\{1,2,3\}$, 
Some examples of subsets

1. $\emptyset$
2. $\{1,2\}$
3. $\{1,2,3\}$

Some nonexamples of subsets

1. $\{\emptyset\}$
2. $1$
3. $\{1,\{2\}\}$

$\mathcal P (A) = \{\emptyset,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\}$

Note that in some older references, $\mathcal P(A)$ is sometimes denoted as $2^A$, could you think why? (Hint: size)

--------------

## Operations on sets 
Let $A$ and $B$ be sets:

### Union
We write $A\cup B$ to be the union of $A$ and $B$: $$A\cup B:=\{x|x\in A\lor x\in B\}$$

Let $I$ be an indexing set and for any $i\in I$, $A_i$ is a set, we define $\bigcup_{i\in I}A_i:=\{x|\exists i\in I,x\in A_i\}$

### Intersection
We write $A\cap B$ to be the intersection of $A$ and $B$: $$A\cap B:=\{x|x\in A\land x\in B\}$$


Let $I$ be an indexing set and for any $i\in I$, $A_i$ is a set, we define $\bigcap_{i\in I}A_i:=\{x|\forall i\in I,x\in A_i\}$

### Difference
We write the difference between $A$ and $B$ as $A-B$ or $A/B$:
$$A-B:=\{x|x\in A\land x\notin B\}$$
In many scenarios, will often fix a set $A$ and everything we do will be within this set. We call this set the universe. In this case instead of $A-B$, we write $B^c$ (for complement). For example the universe of real analysis is often $\mathbb R$ and the set of irrational numbers is $\mathbb Q^c$.

### Symmetric difference
Denoted by $A\Delta B$, we write the symmetric difference of $A$ and $B$ to be:
$$A\Delta B=(A-B)\cup(B-A)$$

### Cartesian product

From the axiom of extensionality, $\{a,b\}=\{b,a\}$. However often we need to care about the order of elements. Thus we define ordered pair (2-tuple)


We then inductively define ordered $n$-tuple. For $n$ elements $a_1,...,a_n$, we define $(a_1,...,a_n)$ to be $(a_1,(a_2,...,a_n))$ where $(a_2,...,a_n)$ is the ordered $n-1$-tuple formed by $(a_2,...,a_n)$.

<div class="Definition">{ordered pair}
\[(a,b)=\{\{a\},\{a,b\}\}\]
</div>

#### Theorem

\[[(a,b)=(x,y)]\iff[(a=x)\land(b=y)]\]



-----------------

#### Proof

For the forward direction:
Suppose $(a,b)=(x,y)$, then $\{\{a\},\{a,b\}\}=\{\{x\},\{x,y\}\}$.

Either $a=b$ or $a\ne b$. 

If $a=b$ then $(a,b)=\{\{a\}\}=(x,y)=\{\{x\},\{x,y\}\}$. By ax. of ext., $\{x\}\in\{\{a\}\}$ and $\{x,y\}\in\{\{a\}\}$. Hence $\{x\}=\{a\}$ and $\{x,y\}=\{a\}$, by ax. of ext., $x=a=y$.

Suppose now $a\ne b$, by ax. of ext., $\{a\}\in \{\{x\},\{x,y\}\}$. However, if $\{a\}=\{x,y\}$, by ax. of ext., we must have $x=y=a$. Then $\{\{x\},\{x,y\}\}=\{\{a\}\}$. Then $\{\{a\},\{a,b\}\}=\{\{a\}\}$. Then $\{a,b\}=\{a\}$, then by ax. of ext., $a=b$, a contradiction. Then $\{a\}=\{x\}$, by ax. of ext., $a=x$. Also $\{a,b\}=\{x,y\}$, for otherwise $a=b$. Then $b=x$ or $b=y$. The former contradicts $b\ne a$ via $a=x$. Hence $b=y$.

The other direction is more or less trivial.


#### come back here after lecture 3

Verify $(a_1,...,a_n)=(b_1,...,b_n)\iff\forall i\in\{1,...,n\},a_i=b_i$

----------------

<div class="Definition">{Cartesian product}

Let $A$ and $B$ be sets, the Cartesian product, written as $A\times B$, is defined as:
\[\{(a,b)|a\in A\land b\in B\}\]. 
Instead of $A\times A$, we write $A^2$.
By ax. of ext., $p\in A\times B\iff\exists\in A\exists b\in B,p=(a,b)$.
</div>

-----------------------

### Example

Let $A=\{0,1,2,3,4,5\}$ and $B=\{1,3,5,7,9\}$.

* $A\cup B=\{0,1,2,3,4,5,6,7,8,9\}$
* $A\cap B=\{1,3,5\}$
* $A-B = \{0,2,4\}$
* $B-A = \{7,9\}$
* $A\Delta B=\{0,2,4,7,9\}$
* $A\cup B=(A\cap B)\cup(A\Delta B)$
* $A\times B$ is
  \begin{equation}\nonumber
  \begin{aligned}
  \{&(0,1),(0,3),(0,5),(0,7),(0,9),\\
    &(1,1),(1,3),(1,5),(1,7),(1,9)\\
    &(2,1),(2,3),(2,5),(2,7),(2,9),\\
    &(3,1),(3,3),(3,5),(3,7),(3,9)\\
    &(4,1),(4,3),(4,5),(4,7),(4,9),\\
    &(5,1),(5,3),(5,5),(5,7),(5,9)\}
  \end{aligned}
  \end{equation}
  I regret here I chose $A$ and $B$ in this way.


-------------------

## Laws for operations on sets

Let $A$, $B$ and $C$ be sets

### Commutative law

1. of union: $A\cup B=B\cup A$
2. of intersection: $A\cap B = B\cap A$

<div class="Proof">
\nonumber

\begin{align}
x \in A\cup B &\iff x\in \{t| t\in A\lor t\in B\}\\
              &\iff x\in A \lor x\in B\\
              &\iff x\in B \lor x\in A\\
              &\iff x\in \{t| t\in B\lor t\in A\}\\
              &\iff x\in B\cup A
\end{align}

By axiom of extensionality, this conclude the proof of 1; 2 is left as an exercise.
</div>

----------------------

### Associative laws (whose proves are left as exercise)

1. of union: $A\cup(B\cup C)=(A\cup B)\cup C$
2. of intersection: $A\cap(B\cap C)=(A\cap B)\cap C$


### Distributive laws

1. $A\cap(B\cup C)=(A\cap B)\cup (A\cap C)$
2. $A\cup(B\cap C)=(A\cup B)\cap (A\cup C)$

<div class="Proof">\nonumber
\begin{align}
x \in A\cap(B\cup C) &\iff x\in A\land(x\in B\cup C) \\
                     &\iff x\in A\land(x\in B\lor x\in C)\\
\end{align}
Please continue using lecture 1.
Proof of 2 is left as exercise.
</div>


--------------------

### Idempotent laws

1. $A\cup A=A$
2. $A\cap A=A$

<div class="Proof">
\nonumber
\begin{align}
a\in A\cup A &\iff a\in \{x|x\in A\lor x\in A \} \\
             &\iff a\in A\lor a\in A \\
             &\iff a \in A
\end{align}
By the axiom of extensionality, we have $A\cup A=A$. Proof of 2 is exercise.
</div>

---------------

### Exercise (a bit more involved)
Prove the following

1. $A\cup (B-A) = A\cup B$ (Hint: use $x\in A\lor x\notin A$ is a tautology.)
2. $A-B=A-(A\cap B)$ (Hint: use $x\in A\land x\notin A$ is a contradiction)
3. De Morgan's Laws (Hint: use De Morgan's law from lecture 1)
   * $A-(B\cap C)=(A-B)\cup(A-C)$
   * $A-(B\cup C)=(A-B)\cap(A-C)$

------------------

### Laws about Cartesian product

Let $A$, $B$, $C$ and $D$ be sets

1. $A\times(B\cup C)=(A\times B)\cup(A\times C)$
2. $A\times(B\cap C)=(A\times B)\cap(A\times C)$
3. $(A\times B)\cup(C\times D)=(A\cup C)\times(B\cup D)$
4. $(A\times B)\cap(C\times D)=(A\cap C)\times(B\cap D)$

----------

#### Proof

1. \[
\begin{aligned}
(x,y)\in A\times(B\cup C) &\iff x\in A\land y\in B\cup C \\
                          &\iff x\in A\land(y\in B\lor y\in C)
\end{aligned}
\]
You can take it from here using lecture 1;
2. similar to 1;

3. \[
\begin{aligned}
(x,y)\in (A\times B)\cup(C\times D) \iff& (x,y)\in (A\times B)\\
                                    \lor& (x,y)\in (C\times D)\\
                          \iff& (x\in A\land y\in B)\\
                          \lor&(x\in C\land y\in D)
\end{aligned}
\]
You can take it from here using lecture 1;
4. similar to 3.


## Relations 

<div class="Definition">
Let $X$ and $Y$ be sets. A (binary) relation from $X$ to $Y$ is simply a subset $R\subseteq X\times Y$. When dealing with relations, instead of $(x,y)\in R$ we write $xRy$.

An $n$-ary relation amongst $A_1$,...,$A_n$ is a subset of $A_1\times...\times A_n$.
</div>

Let $R$ be a binary relation from $X$ to $Y$, we define

1. The domain of R, $\mathrm{dom}R:=\{x\in X|\exists y\in Y,xRy\}$
2. The range of R,$\mathrm{ran}R:=\{y\in Y|\exists x\in X,xRy\}$
3. The inverse relation of $R^{-1}:=\{(y,x)\in Y\times X|xRy\}$

### Example
The "$<$" relation on $\mathbb N$ is the set $\{(0,1),(0,2),...,(1,2),(1,3),...\}$.

* $\mathrm{dom}(<)=\mathbb N$
* $\mathrm{ran}(<)=\mathbb N-\{0\}$
* inverse of $<$ is $\{(1,0),(2,0),...,(2,1),(3,1),...\}$, the $>$ relation on $\mathbb{N}$

-------------


We can compose relations: let $R$ be a relation from $X$ to $Y$ and $S$ be a relation from $Y$ to $Z$. The composition of $S$ and $R$ is a relation from $X$ to $Z$ defined as:
\[S\circ R:=\{(x,z)\in X\times Z|\exists y\in Y,xRy\land ySz\}\]

Let $R$, $S$, $T$ be relations from $A$ to $B$, $B$ to $C$, $C$ to $D$ respectively, we have the following:

1. $(R^{-1})^{-1}=R$
2. $\mathrm{dom}(R^{-1})=\mathrm{ran}R$
3. $\mathrm{ran}(R^{-1})=\mathrm{dom}R$
4. $T\circ(S\circ T)=(T\circ S)\circ R$
5. $(S\circ R)^{-1}=R^{-1}\circ S^{-1}$

Try this or otherwise see @stewart2015foundations for proves.

-------------------

Let $R$ be a relation from $A$ to $A$. R is

- reflexive if and only if $\forall x\in X,xRx$;
- symmetric if and only if $\forall x\in X\forall y\in X,xRy\implies yRx$
- transitive if and only if $\forall x\in X\forall y\in X\forall z\in X,(xRy\land yRz)\implies xRz$

If R is reflexive, symmetric and transitive then R is called an equivalence relation. In this case we sometimes write $xRy$ as $x\equiv y\textrm{ mod }R$.

- $\le$ on $\mathbb Z$ is reflexive, transitive but not symmetric.
- $=$ is an equivalence relation.
- $\{(x,y)\in \mathbb Z^2||x-y|\le 1\}$ is reflexive, symmetric but not transitive.
- $\equiv\textrm{mod }m:=\{(x,y)\in\mathbb Z^2|\exists k\in \mathbb Z, x-y=km\}$ is an equivalence relation.

------

<div class="Definition">
Let $R$ be an *equivalence* relation on $X$ and $x\in X$. The equivalence class of $x$ with respect to $R$, written as $[x]_R$, is $\{y\in X|(y,x)\in R\}$ 

</div>

Trivially, since $R$ is in particular reflexive, $\forall x\in X,xRx$ hence $\forall x,x\in[x]_R$.

--------

*The following is really quite important*. The proof is trivial but equivalence relation is ubiquitous in mathematics.

#### Theorem

$\forall x\in X,\forall y\in X, y\in[x]_R\iff([y]_R=[x]_R)$

### Proof
Fix $x\in X$ and $y\in X$.
The forward direction: suppose $y\in[x]_R$. Then for any $z$
\[
\begin{aligned}
z\in[y]_R&\iff yRz & [\textrm{by defition}]\\
         &\iff xRz & [R\textrm{ is equivalence relation}^\dagger]\\
         &\iff z\in[x]_R &[\textrm{by definition}]
\end{aligned}
\]

The other direction is trivial.

For lack of time, the $\dagger$ part is not explained with entirety, but do it to test if you understand transitivity and symmetry. Since equivalence relation is important, read more on @stewart2015foundations.

## Function 

Structurally speaking, function is just a special type of relation, but we seldom think it of this way, rather we tends to think function as if a rule of assignment.

<div class="Definition">
Let $f$ be a relation from $X$ to $Y$. We say $f$ is a function from $X$ to $Y$ if and only if
\[\forall x\in X,\exists!y\in Y,(x,y)\in f\land \mathrm{dom}f=X\]
</div>

In this case instead of $(x,y)\in f$ or $xfy$, we write it as $f(x)=y$ or $y\mapsto f(x)$; we also note $f$ being a function from $X$ to $Y$ as $f:X\to Y$. We call $Y$ as the codomain $f$, denoted as $\mathrm{codom} f$. Note $\mathrm{codom}$ is different from $\mathrm{ran}$.

For any set $X$, we have a function usually called the identity on X $i_X:=\mathrm{diag}X$ from $X$ to $X$.

---------

#### Theorem

Let $X$, $Y$ be sets and $f$, $g$ on functions from $X$ to $Y$. Then
\[f=g\iff\forall x\in X,f(x)=g(x)\]

#### Proof
Forward direction is trivial.
Let $x\in X$ since $f$ is a function, there is a unique $y\in Y$ such that $y=f(x)$, then $(x,y)\in g$. This is $f\subseteq g$. Similarly $g\subseteq f$. So for functions $f, g$ to be equal, domains of $f$ and $g$ must equal and codomains must be equal as well.

So if $f:\mathbb Z\to\mathbb Z$, $g:\mathbb Z\to \mathbb Z$,$h:\mathbb Z\to\mathbb N$ be given by $f(x)=(x+1)^2$, $g(x)=x^2+2x+1$ and $h(x)=(x+1)^2$ respectively, then $f=g$ but $f\ne h$ for their codomains are different.


-----------------

#### Theorem

For $f:X\to Y$,$g:Y\to Z$, we have $g\circ f$ is a function and $\forall x\in X,g\circ f(x)=g(f(x))$

#### Proof
Recall that $g\circ f:=\{(x,z)|\exists y\in Y,(x,y)\in f\land (y,z)\in g\}$. This is $g\circ f:=\{(x,z)|\exists y\in Y, y=f(x)\land z=g(y)\}$. Since $f$ is a function, this is $g\circ f:=\{(x,z)|\exists!y\in Y,y=f(x)\land z=g(y)\}$. So for any $x\in X$, suppose there are $z_1$ and $z_2$ in $Z$ such that $(x,z_1)$ and $(x,z_2)$ are both in $g\circ f$, then there is a unique $y\in Y$ such that $y=f(x)$, but $z_1=g(y)=z_2$. This concludes the proof.

----------------

### Example

1. Let $X=\{1,2,3\}$, $Y={4,5,6}$, $f=\{(1,4),(2,5),(3,6)$ is a function. We more customarily write it as \[f:\{1,2,3\}\to\{4,5,6\}\]\[x\mapsto x+3\] In contrast, $g=\{(1,4),(1,5),(2,6),(3,6)\}$ is not a function.

2. $f=\{(x,y)\in \mathbb R^2|y=x^2\}$ is a function from $\mathbb R$ to $\mathbb R$. In contrast $g_1=\{(x,y)\in \mathbb R^2|x=y^2\}$ is not a function from $\mathbb R$ to $\mathbb R$. However, $g_2=\{(x,y)\in \mathbb R\times \mathbb R_{\ge0}|x=y^2\}$ is a function from $\mathbb R$ to $\mathbb R_{\ge0}$, the non-negative real numbers. 

3. Let $f:\mathbb Z\to\mathbb Z$ and $g:\mathbb Z\to\mathbb Z$ be given by $f(x)=x^2+1$ and $g(x)=2x-1$ respectively. Then $f\circ g:\mathbb Z\to\mathbb Z$ and $g\circ f:\mathbb Z\to\mathbb Z$ are both functions given by $f\circ g(x)=f(g(x))=g(x)^2+2=(2x-1)^2+2=4x^2-4x+3$ and $g\circ f(x)=g(f(x))=2f(x)-1=2x^2+3$. So composition is *not* commutative.

--------------------

### Generalized Cartesian product

For an indexing set $I$ such that for any $i\in I$,$A_i$ is a set. We can define the cartesian product of $\{A_i|i_\in I\}$, denoted by $\prod_{i\in I} A_i$, to be $\{f:I\to\bigcup_{i\in I}A_i|\forall i\in I,f(i)\in A_i\}$. Think about this -- in which sense this is a generalization of the original notion. (Hint: think about the set $\{1,2\}$)

### Axiom of choice

In the above notion, if $I\ne\emptyset$ and $\forall i\in I,A_i\ne\emptyset$, then $\prod_{i\in I}A_i$ is not empty.

Axiom of choice is an axiom of vital importance, but it has some consequences weird enough for it to be debatable. Try google Banach-Tarski paradox or look at @halmos2017naive. Really, it is fun.

---------------

### Image and inverse image

<div class="Definition">
Let $f:X\to Y$, $A\subseteq X$ adn $B\subseteq Y$. By $f(A)$, the image of $A$ under $f$. we mean $\{y\in Y|\exists x\in A,y=f(x)\}$; by $f^{-1}(B)$, the preimage of $B$ of $f$, we mean $\{x\in X|\exists y\in B,y=f(x)\}$.
</div>

Not important to us, but we have two induced function: 

1. $\mathrm{im}:\mathcal P(X)\to\mathcal P(Y)$ given by $A\mapsto f(A)$;
2. $\mathrm{preim}:\mathcal P(Y)\to\mathcal P(X)$ given by $B\mapsto f^{-1}(B)$

#### Theorem

Let $f:X\to Y$, $A_1, A_2\subseteq X$ and $B_1, B_2\subseteq Y$, then

1. $\mathrm{ran}f=f(X)$;
2. $f(A_1\cup A_2)=f(A_1)\cup f(A_2)$;
3. $f(A_1\cap A_2)\subseteq f(A_1)\cap f(A_2)$;
4. $A\subseteq B\implies f(A)\subseteq f(B)$
5. $f^{-1}(B_1\cup B_2)=f^{-1}(B_1)\cup f^{-1}(B_2)$
6. $f^{-1}(B_1\cap B_2)=f^{-1}(B_1)\cap f^{-1}(B_2)$
7. $B_1\subseteq B_2\implies f^{-1}(B_1)\subseteq f^{-1}(B_2)$.

-----------


#### Proof

1. By definition;
2. 
\[
\begin{aligned}
y\in f(A_1\cup A_2)
               \iff&\exists x\in A_1\cup A_2,y=f(x)\\
               \iff&\exists x,x\in A_1\cup A_2\land y=f(x)\\
               \iff&\exists x,(x\in A_1\lor x\in A_2)\land y=f(x)\\
               \iff&\exists x, (x\in A_1\land y=f(x))\\&\lor(x\in A_2\land y=f(x))\\
               ^\dagger\iff&(\exists x, (x\in A_1\land y=f(x)))\\&\lor(\exists x, (x\in A_2\land y=f(x)))\\
               \iff&(\exists x\in A_1,y=f(x))\\&\lor(\exists x\in A_2,y=f(x))\\
               \iff&x\in f(A_1)\cup f(A_2)
\end{aligned}\]

The rest are exercises. Pay special attention to 3 to see why the step corresponding to $\dagger$ doesn't work, or mayber carefully think why $\dagger$ in 2 works.

----------------

### Injction, surjection and inverse functions.

Let $f:X\to Y$ be a function

<div class="Definition">
$f$ is an injection (also one-to-one or monomorphism) if and only if:
\[
\forall x_1\in X,\forall x_2\in X,[(f(x_1)=f(x_2))\implies x_1=x_2];
\]
$f$ is an surjection (also onto or epimorphism) if an only if $\mathrm{ran}f=Y$, equivalently
\[
\forall y\in Y,\exists x\in X,y=f(x)
\]
If $f$ is both injective and surjective, $f$ is a bijection (or isomorphism).
</div>


#### Remark
If $f:X\to Y$ is an injection, then $\bar f:X\to\mathrm{ran}f$ defined by $x\mapsto f(x)$ is a bijection.

-----------

#### Example

1. For any set $X$, $i_X$ is a bijection.
2. Let $X = \{1,2,3,4\}$,$Y = \{5,6,7\}$. Define $f:X\to Y$ as $f:=\{(1,5), (2, 6), (3, 7), (4, 6)\}$. Then $f$ is an surjection. But define $g:X\to Y$ as $g:=\{(1,5), (2, 6), (3, 5), (4, 6)\}$, then g is not surjective, for example $7\notin\mathrm{ran}g$
3. Let $f :\mathbb N \to\mathbb N$ defined by $f(n) = n^2$. Then $f$ is an injection.
In contrast let $g : \mathbb Z \to \mathbb Z$ defined by $g(n) = n^2$. Then $g$ is not an injection.

----------

### Inverses

Let $f:X\to Y$ be a function.
<div class="Definition">
A function $g:Y\to X$ is said to be a left inverse (or retraction) of $f$ if and only if $g\circ f=i_X$;

A function $h:Y\to X$ is said to be a right inverse (or section) of $f$ if and only if $f\circ h=i_Y$.

A function $j:Y\to X$ is said to be an inverse of $f$ if and only if $f\circ j=i_Y$ and $j\circ f=i_X$. In particular, an inverse is both a left inverse and a right inverse.
</div>

------------

#### Theorem

1. If a left inverse of $f$ exists, then $f$ is injective;
2. If a right inverse of $g$ exists, then $f$ is surjective;
3. If an inverse exists then $f$ is bijective;

#### Proof

1. Fix $x_1$, $x_2$ to be two elements of $X$. Suppose $f(x_1)=f(x_2)$, then $g\circ f(x_1)=g\circ f(x_2)$ then by g being left inverse, $x_1=x_2$.
2. For any $y\in Y$, $f(h(y))=y$.
3. 1 and 2.


------------

Assume $X$ and $Y$ are nonempty and $f:X\to Y$ is a function

#### Theorem

1. If $f$ is injective, then a left inverse exists;
2. If $f$ is surjective, then a right inverse exists;
3. If $f$ is bijective, then an inverse exists;

#### Proof

1. Fix $a\in X$. Define $g:X\to Y$ by
\[
y\mapsto\left\{
\begin{aligned}
  x &\ \ \ \exists x\in X,y=f(x)^\dagger\\
  a &\ \ \ y\notin\mathrm{ran}f\\
\end{aligned}\right.
\]

This is well-defined because of injectivity of $f$: the $x$ is unique in $\dagger$.

Then $g\circ f(x)=x$

2. By surjectivity of $f$, for any $y\in Y$, $f^{-1}(\{y\})$ is non-empty and $Y=\bigcup_{y\in Y}f^{-1}(\{y\})$. By the axiom of choice, there is a function $h:Y\to X$ such that for any $y\in Y$, $h(y)\in f^{-1}(\{y\})$.

3. The $g$ in part 1 is actually also a right inverse.

------------

#### Theorem

If $f:X\to Y$ is a bijective, then the inverse function is unique. Henceforward we denote it as $f^{-1}$. 

#### Proof

Suppose $g_1:Y\to X$ and $g_2:Y\to X$ are two inverse functions of $f$. Then for any $g_1=g_1\circ i_Y=g_1\circ(f\circ g_2)=(g_1\circ f)\circ g_2=i_X\circ g_2=g_2$.


----------------------

### Exercise

- Let $\mathfrak F\subseteq \mathcal P(\mathbb N)$ be the collection of all finite subsets of natural number. Define a relation on $\mathfrak F$ to be $S_1\approx S_2\iff\exists f:S_1\to S_2, f\textrm{ bijective}$. 

1. Prove $\approx$ is an equivalence relation.
2. What is $[\emptyset]_\approx$?
3. In general, what is $[\{1,2,...,n\}]_\approx$

- [Hard] Prove that if there is an injective function $f:X\to Y$ and an injective function $g:Y\to X$, then there is a bijective function $h:X\to Y$. If you find this too hard, read and understand the Schröder–Bernstein theorem.

------------------

## Summary

In this lecture we covered basic naïve theory and defined notion of relation and function. Perhaps the most important two points (not just for this course) you should take are the notion of equivalence relation and the notion of bijective function.


# Mainly $\mathbb N$ and $\mathbb Z$ (Lecture 3)

In this lecture, we'll be talking about the construction of natural numbers, integers and rational numbers. There arithmetic will be defined. In the end you can basically cover everything you already knew about natural numbers integers and rational numbers, but now you can say with (more) confidence you actually know what a natural number, integer and rational number are.

## $\mathbb N$ 

Without the time to do properly the full picture, but here it comes:

<div class="Definition">

A set $x$ is inductive if:
- $\emptyset\in x$
- $\forall a\in x, a\cup\{a\}\in x$

$a\cup\{a\}$ is called the successor of $a$, sometime we write it as $S(a)$.

</div>

Another axiom of set theory is the axiom of infinity asserting existence of an inductive set.

We have the following theorem
<div class="Theorem">
For any two set $x$ and $y$, if $x$ and $y$ are both inductive, then so is $x\cap y$
</div>

<div class="Proof">
$\emptyset \in x\cap y$ because $\emptyset\in x$ and $\emptyset\in y$. Suppose $a\in x\cap y$, then since $a\in x$ ($\in y$, resp.) we have $a\cup\{a\}\in x$ ($\in y$ resp.), thus $a\cup\{a\}\in x\cup y$.
</div>

Let $x$ be the inductive set endorsed by the axiom of infinity, we define the natural number as the smallest inductive subset of $x$ $\bigcap_{y\in\mathcal P(x) \textrm{inductive}} y$

This intimidating definition is actually quite nice for $\mathbb N$ being inductive must contain $\{\emptyset, S(\emptyset), S(S(\emptyset)) , ...\}$ and $\{\emptyset, S(\emptyset), S(S(\emptyset)) , ...\}$ is indeed inductive hence we have $\mathbb N = \{\emptyset, S(\emptyset), S(S(\emptyset)) , ...\}$. Still intimidating? (This is only an aid for clarification) Maybe instead of $\emptyset$, we write it as $0$ and instead of $S(\emptyset)$ we write it as $1$ etc. Then

0. $0 = \emptyset$
1. $1 = S(0) = S(\emptyset)=\emptyset\cup\{\emptyset\}=\{\emptyset\}=\{0\}$
2. $2 = S(1) = 1\cup\{1\}=\{0\}\cup\{1\}=\{0,1\}$
3. In general, $n+1 = \{0,...,n\}$

So $n$ is a set with $n$ elements. In particular $0\ne S(n)$ for any $n$. It requires more set theory than that I would like to develop [^3] to prove $S$ is injective, i.e $S(a)=S(b)\implies a=b$, but this is true.

Now we can show the principle of induction:
<div class="Theorem">
Let $P$ be some property of natural number, if $P(0)$ and $\forall m\in\mathbb N,P(m)\implies P(m+1)$ then all natural number satisfies $P$.
</div>

<div class="Proof">
Let $y:=\{n\in\mathbb N|P(n)\}$. Then $y$ is inductive: $0\in y$ and for any $m\in y$, since $P(m)$ we have $P(m+1)$ hence $m+1\in y$. So $y$ is an inductive subset of $\mathbb N$ who is the smallest inductive subset. Hence $y=\mathbb N$. In otherword, all natural number has the property $P$. 
</div>

For every $n \in\mathbb N$, we can define an "adding n" function, for convenience we write it as $n+$:
1. $n + 0 := n$
2. $n + S(m) := S(n+m)$


For example the function "adding 0" does nothing, and we prove this by induction:

1. $0+0=0$ by definition
2. Suppose $0+m=m$, then $0+S(m)=S(0+m)=S(m)$

From this we can prove the commutativity: $\forall n\in\mathbb N\forall m\in\mathbb N,n+m=m+n$.

1. From above we proved $\forall m\in\mathbb N,0+m=m=m+0$
2. Suppose $\forall m\in\mathbb N,n+m=m+n$. Let's now prove $\forall m\in\mathbb N,S(n)+m=m+S(n)$ by induction on $m$:
  * $S(n)+0=S(n)=0+S(n)$
  * Suppose $S(n)+m=m+S(n)$, then $S(n)+S(m)=S(S(n)+m)=S(m+S(n))=S(S(m+n))$. Similarly $S(m)+S(n)=S(m+S(n))=S(S(m+n))$.

Associativity is proved in a similar (but more sophisticated) manner.

The function "adding $n$" is injective: if $n+a=n+b$ then $a=b$. This is more wildly known as the cancellation property.
- "adding $0$" is injective, indeed: $0+a=0+b$ by definition is $a=b$.
- Suppose "adding $n$" is injective then $S(n)+a=S(n)+b\implies a+S(n)=b+S(n)\implies S(a+n)=S(b+n)\implies a+n=b+n\implies n+a=n+b\implies a=b$.


Similarly we can define a function "multiplying $n$", denote it by $n\times$:
1. $n\times 0:=0$
2. $n\times S(m) := n\times m+ n$

So for example $n\times 1=n\times S(0)=n\times0+n=n$.
We can of course prove the commutativity and asscociativity for multiplication. These are left as exercises.

From addition we can define "less than" ($<$) and and "divisibility" ($|$) amongst other things: Let $m$, $n$ be natural numbers

1. $n < m\iff\exists c\in\mathbb N,m=n+c$
2. $n\le m\iff n<m\lor n=m$
2. $n | m\iff\exists k\in\mathbb N,m=n\times k$.

Next important theorem is the well-ordering property of natural number.
<div class="Theorem">
Let $s$ be an non-empty subset of natural number, then $s$ has a least element. This is $\exists n_0\in s,\forall m\in s,n_0 \le m$.
</div>

<div class="Proof">
We proof by contradiction. Suppose $s$ is non-empty without a least element. Define $Z:=\{k\in\mathbb N|\forall n\in\mathbb N,(n<k)\implies(n\notin s)\}$. Then $0\in Z$. Suppose $k\in Z$, let $m<S(k)$ then either $m<k$ or $m=k$. If $m<k$, by $k\in Z$ we have $m\notin s$. If $m=k$,then $m\notin s$ for otherwise $m$ is the least element of $s$. Hence as long as $m< S(k)$, we have $m\notin s$. Hence $Z$ is inductive. Then $Z=\mathbb N$. Then $s=\emptyset$, this is a contradiction.
</div>

From the well-ordering property, we have the strong induction principle:
<div class="Theorem">
Let $P$ be a property about natural numbers. Suppose $P(0)$ and for any natural number $n$, $P(0)\land P(1)\land...\land P(n)\implies P(n+1)$. Then all natural numbers have property $P$.
</div>

<div class="Proof">
Let $s:=\{n\in\mathbb N|\neg P(n)\}$. Then if $s$ is non-empty, $s$ will have least element $n_0$. $n_0$ cannot be $0$, since $P(0)$. Then $P(1)$,....,$P(n_0-1)$ but $\neg P(n_0)$. Contradiction. Hence $s=\emptyset$.
</div>

In fact well-ordering principle $\iff$ strong induction. Please fill in the other direction.

## Example and practice using induction

<div class="Example">
\[\sum_{i=0}^n i^2=\frac{n(n+1)(2n+1)}{6}\]
</div>


- For $n=0$, left hand side is $0$ and so the right hand side.
- Suppose $\sum_{i=0}^n i^2=\frac{n(n+1)(2n+1)}{6}$ then $\sum_{i=0}^{n+1} i^2=\sum_{i=0}^n i^2+(n+1)^2=\frac{n(n+1)(2n+1)}{6}+(n+1)^2=\frac{n(n+1)(2n+1)+6(n+1)^2}{6}=\frac{(n+1)(n(2n+1)+6(n+1))}{6}=\frac{(n+1)(n+2)(2n+3)}{6}$

----------------

<div class="Example">
\[\sum_{i=0}^n (2i+1)=(n+1)^2\]
</div>


- For $n=0$, left hand side is $1$ is indeed $(0+1)^2$.
- Suppose $\sum_{i=0}^n (2i+1)=(n+1)^2$ then $\sum_{i=0}^{n+1} (2i+1)=(n+1)^2+(2n+3)=n^2+2n+1+2n+3=n^2+4n+4=(n+2)^2$


Let $0!=1$ and $(n+1)!=n!\times(n+1)$.
Let ${n\choose k}=\frac{n!}{(n-k)!k!}$ for $k\le n$ and $0$ for $k>n$


--------------


<div class="Example">
$${n\choose k+1}+{n\choose k}={n+1\choose k+1}$$
</div>

- For $k>n$ both lhs, rhs are 0.
- For $k=n$ then lhs is $0+1$ while rhs is 1.
- For $k<n$ then


\begin{equation}\nonumber
\begin{aligned}
{n+1\choose k+1}&=\frac{(n+1)!}{(k+1)!(n-k)!}\\
{n\choose k+1}+{n\choose k}&=\frac{n!}{(n-k-1)!(k+1)!}+\frac{n!}{k!(n-k)!}\\
                &=\frac{n!(n-k)+n!(k+1)}{(k+1)!(n-k)!}
\end{aligned}
\end{equation}

--------------

<div class="Example">
\[(a+b)^n=\sum_{i=0}^n{n\choose i}a^ib^{n-i}\]
</div>

- $n=0$, lhs is $1$; rhs is ${0\choose 0}a^0b^0=1$.
- Suppose $(a+b)^n=\sum_{i=0}^n{n\choose i}a^ib^{n-i}$, then

\begin{equation}\nonumber
  \begin{aligned}
  (a+b)^{n+1}&=\left (\sum_{i=0}^n {n\choose i}a^i b^{n-i}\right)\times(a+b)\\
             &=\sum_{i=0}^n{n\choose i}a^{i+1}b^{n-i} + \sum_{i=0}^n{n\choose i}a^ib^{n-i+1} \\
             &=a^{n+1}+\sum_{i=0}^{n-1}{n\choose k}a^{k+1} b^{n-k}+b^{n+1}+\sum_{i=1}^n{n\choose k}a^k b^{n-k+1}\\
  \end{aligned}
\end{equation}

-----------------

\begin{equation}
\nonumber
\begin{aligned}
  (a+b)^{n+1}&=a^{n+1}+\sum_{i=0}^{n-1}{n\choose k}a^{k+1} b^{n-k}+b^{n+1}+\sum_{i=0}^{n-1}{n\choose k+1}a^{k+1} b^{n-k}\\
             &=a^{n+1}+b^{n+1}+\sum_{i=0}^{n-1}{n+1\choose k+1}a^{k+1}b^{n-k}\\
             &=a^{n+1}+b^{n+1}+\sum_{i=1}^{n}{n+1\choose k}a^{k}b^{n+1-k}\\
             &=\sum_{i=0}^{n+1}{n+1\choose k}a^{k}b^{n+1-k}
\end{aligned}
\end{equation}


-----------------

Define the fibbonacci number to be $F_0 = 0$, $F_1=1$ and $F_n = F_{n-1}+F_{n-2}$. Write $\phi = \frac{\sqrt 5-1}{2}$

<div class="Example">
  $$F_n=\frac{1}{\sqrt 5}\left(\left(\frac{1+\sqrt 5}{2}\right)^n-\left(\frac{1-\sqrt 5}{2}\right)^n\right)$$  
</div>

In this example, we need to use strong induction.

- $n=0$, check
- $n=1$, rhs = $\frac{1}{\sqrt 5}\left(\frac{1+\sqrt 5-1+\sqrt 5}{2}\right)=1$

---------

- Suppose hold for $0$ up to $n$. Then write $\phi = \frac{1+\sqrt 5}{2}$. Then $F_n=\frac{1}{\sqrt 5}\left(\phi^n-(-\frac{1}{\phi})^n\right)$ and $F_{n-1}=\frac{1}{\sqrt 5}\left(\phi^{n-1}-(-\frac{1}{\phi})^{n-1}\right)$. Note $\phi ^ {n+1}-\phi^n-\phi^{n-1}=\phi^{n-1}\times(\phi^2-\phi-1)=\phi^{n-1}\times(\frac{3+\sqrt 5}{2}-\frac{1+\sqrt 5}{2}-1)=0$. Similarly $\left(-\frac{1}{\phi}\right)^{n+1}-\left(-\frac{1}{\phi}\right)^n-\left(-\frac{1}{\phi}\right)^{n-1}=\left(-\frac{1}{\phi}\right)^{n-1}\times\left(\left(-\frac{1}{\phi}\right)^2-\left(-\frac{1}{\phi}\right)-1\right)=\left(-\frac{1}{\phi}\right)^{n-1}\times\left(\frac{2}{3+\sqrt 5}+\frac{2}{\sqrt 5+1}-1\right)=\left(-\frac{1}{\phi}\right)^{n-1}\times\left(\frac{3-\sqrt5}{2}+\frac{\sqrt 5-1}{2}-1\right)=0$. Hence we have $F_{n+1}=\frac{1}{\sqrt 5}\left(\left(\frac{1+\sqrt 5}{2}\right)^{n+1}-\left(\frac{1-\sqrt 5}{2}\right)^{n+1}\right)$

-----------

## $\mathbb Z$

### Definition 

This is the first descent application of equivalence relation.

Let us define a relation $\sim$ on $\mathbb N\times\mathbb N$ to be $(a,b)\sim(x,y)\iff a+y=b+x$.

<div class="Theorem">
$\sim$ is an equivalence relation.
</div>

- reflexivity: for all $(a,b)\in\mathbb N^2$, since $a+b=b+a$, we must have $(a,b)\sim(a,b)$.
- symmetry: $(a,b)\sim(x,y)\iff a+y=b+x\iff x+b=y+a\iff(x,y)\sim(a,b)$.
- transitivity: $(a,b)\sim(x,y)\land(x,y)\sim(\phi,\psi)$ then $a+y=b+x\land x+\psi=y+\phi$ then $a+y+\phi=b+x+\phi$ then $a+x+\psi=b+x+\phi$ then $a+\psi = b+\phi$.

<div class="Definition">
$\mathbb Z$ is the set of equivalence classes under $\sim$. In maths jibber jabber this is $\mathbb N^2$ modulo $\sim$, written as $\frac{\mathbb{N}^2}\sim$.
</div>


-------

Let us look at some examples of elements in $\mathbb Z$

- $[(3,0)]_\sim$ is also $[(4,1)]_\sim$ or $[(5,2)]_\sim$ etc.
- $[(0,3)]_\sim$ is also $[(1,4)]_\sim$ or $[(2,5)]_\sim$ etc.
- $[(0,0)]_\sim$ is also $[(1,1)]_\sim$ or $[(2,2)]_\sim$ etc.


<div class="Theorem">
Every elements in $\mathbb Z$ can be written as $[(a,0)]_\sim$ or $[(0,a)]_\sim$. Such an $a$ is unique.
</div>
<div class="Proof">
Say we are talking about $[(x,y)]_\sim$. If $x<y$ then by definition there is some $z$ such that $y=x+z$, then $[(x,y)]_\sim=[(0,z)]_\sim$. If $x=y$, then $[(x,y)]_\sim=[(0,0)]_\sim$. If $y<x$, then for some $z$ we have $y+z=x$ then $[(z,0)]_\sim$ will do.

$[(a,0)]_\sim=[(b,0)]_\sim\implies(a,0)\sim(b,0)\implies a=b$. The other case is basically the same.
</div>

So now the definition may be clearer: $[(a,0)]_\sim$ are the "positive integers" and $[(0, a)]_\sim$ are the "negative integers".

----------

### Arithmetic on $\mathbb Z$

#### Addition and subtraction

Let $[(a,b)]$ and $[(x,y)]$ be integers

We define $[(a,b)]+[(x,y)]=[(a+x, b+y)]$. We need to check whether this is well-defined:
Suppose $(a,b)\sim(a',b')$ and $(x,y)\sim(x',y')$. Then $(a+x,b+y)\sim(a'+x',b'+y')$ because $(a+x)+(b'+y')=(a+b')+(x+y')=(b+a')+(y+x')=(b+y)+(a'+x')$

Some examples:

- $[(1,0)]+[(0,1)]=[(1,1)]=[(0,0)]$. (Informally $1+(-1)=0$)
- $[(0,1)]+[(0,2)]=[(0,3)]$ (Informally $(-1)+(-2)=-3$)
- $[(4,5)]+[(12,6)]=[(16,11)]=[(5,0)]$. (Informally $(-1)+6=5$)


We can define $[(a,b)]-[(x,y)]=[(a,b)]+[(y,x)]$. (Informally $z_1-z_2=z_1+(-z_2)$). Thankfully we checked addition is well defined, we don't have to check again.

We can of course check commutativity associativity of addition on $\mathbb Z$ and so on.

---------

#### Multiplication

We define $[(a,b)]\times[(x,y)]=[(ax+by,ay+bx)]$. We also need to check whether this is well-defined:
Suppose $(a,b)\sim(a',b')$ and $(x,y)\sim(x',y')$. then $x(a+b')+y(a'+b)+a'(x+y')+b'(x'+y)=x(a'+b)+y(a+b')+a'(x'+y)+b'(x+y')$. Expand out and cancel what can be cancelled we get $ax+by+a'y'+b'x'=bx+ay+a'x'+b'y'$. Rearrange what get what we want.

Some examples:

- $[(1,1)]\times[(1,0)]=[(1,1)]=[(0,0)]$. (Informally $0\times1=0$)
- $[(0,1)]\times[(0,1)]=[(1,0)]$. (Informally $(-1)\times(-1)=1$)


--------

#### Order

Let $x$ be an element in $\mathbb Z$. We say $x\ge 0$ ($x$ is non-negative) if $x=[(k,0)]$ for some $k\in\mathbb N$. We say $x\le 0$ ($x$ is non-positive) if $x=[(0,k)]$ for some $k\in\mathbb N$. From previous theorem, this is always possible.

Thus we can define an order on $\mathbb Z$: for any two integers $x$ and $y$, $x\le y\iff y-x\le 0$.

<div class="Theorem">
Let $x$ and $y$  be two non-positive integer then $x\times y$ is non-negative.
</div>
<div class="Proof">
So for some $k_1,k_2\in\mathbb N$, $x=[(0,k_1)]$ and $y=[(0,k_2)]$. Then $x\times y$ be definition is $[(k_1\times k_2,0)]$, so non-negative.
</div>

So you must have seen $[(k,0)]$ is just $k$ and $[(0,k)]$ is just $-k$ like what we learnt in primary school.

-----------

#### Exercise

1. Let $x,y,z$ be integers, prove $x(y+z)=xy+xz$.
2. Let $a,b$ be integers, prove $a-b+b=a$
3. Google what is integral domain and prove that $\mathbb Z$ is an integral domain. If too boring, assume $\mathbb Z$ is a ring, so just prove $xy=0\implies x=0\lor y=0$

---------


## Field

<div class="Definition">
A field is a set $F$ with addition and multiplication on $F$ such that 

\begin{enumerate}
\item addition and multiplication is associative and commutative;
\item there are two distinctive elements $0$ and $1$ such that $\forall a,a+0=a\land a\times1=1$;
\item for every $a\in F$, there is $(-a)$ and $a^{-1}$ such that $a+(-a)=0$ and $a\times a^{-1}=1$;
\item multiplication distribute over addition.
\end{enumerate}
</div>

So $\mathbb N$ is not a field for $1$ doesn't have additive inverse "$-1\in\mathbb N$"; $\mathbb Z$ is not a field for $2$ doesn't have multiplicative inverse $\frac{1}{2}\in\mathbb Z$. Let's fix this by constructing $\mathbb Q$.

---------------------

## $\mathbb Q$ 

The construction of rational from integers is awfully similar to construction of integers. We define everything but leave everything else as exercise.

<div class="Definition">
We define a relation on $\mathbb Z\times(\mathbb Z-\{0\})$ to be $(a,b)\approx(c,d)\iff ad=bc$.
</div>

<div class="Theorem">
$\approx$ is equivalence relation.
</div>

<div class="Definition">
$\mathbb Q:=\frac{\mathbb Z\times(\mathbb Z-\{0\})}{\approx}$ is the set of equivalence classes.
</div>

Let $[(a,b)]_\approx$ and $[(c,d)]_\approx$ be rational numbers.

- addition is defined as $[(a,b)]_\approx+[(c,d)]_\approx[(ad+bc,bd)]_\approx$;
- $-[(a,b)]_\approx=[(-a,b)]_\approx$. $[(a,b)]_\approx-[(c,d)]_\approx=[(a,b)]_\approx+(-[(c,d)]_\approx)$
- multiplication is defined as $[(a,b)]_\approx\times[(c,d)]_\approx=[(ac,bd)]_\approx$;

<div class="Theorem">
Addition and multiplication is well defined.Hence subtraction is well defined.
</div>

- $[(a,b)]_\approx\ge0$ if and only if $a\times b\ge0$ as an inequality of integers.
- $[(a,b)]_\approx\ge[(c,d)]_\approx$ if and only if $[(a,b)]_\approx-[(c,d)]_\approx\ge0$

In the end instead of $[(a,b)]_\approx$, we write $\frac{a}{b}$ like we did in primary school.

### Big Exercise

Prove that $\mathbb Q$ is indeed a field.


## Summary

So natural number is the smallest inductive set.
So integer is $\frac{\mathbb N^2}{\sim}$.
So rational number is $\frac{\mathbb Z\times(\mathbb Z-\{0\})}{\approx}$.
So we learnt so much yet in a sense nothing new.


# $\mathbb R$ and the completeness axiom (Lecture 4)

In this lecture, we are going to construct the real numbers and prove the completeness axiom.

So first of all, why isn't $\mathbb Q$ enough, after all $\mathbb Q$ is a field already? In some sense $\mathbb Q$ is big enough already. For any two distinct elements $p, q \in \mathbb Q$, there is a third rational number in between, namely $\frac{p+q}{2}$. But in some other sense $\mathbb Q$ is not very big, indeed $\mathbb Q$ is countable (will be explained below), loosely meaning $\mathbb Q$ is only as big as $\mathbb N$. But the main problem is that $\mathbb Q$ has a lot of "holes". Let me explain. $\{x\in\mathbb Q|x^2<2\}$ and $\{x\in\mathbb Q|x^2 > 2\}$ covers $\mathbb Q$ (meaning their union is whole of $\mathbb Q$). However this cover is missing the "$x$" such that $x^2=2$. We will fix this in this lecture.


## Countability

A set $X$ is said to be countable if and only if there is a bijection from $X$ to some subset of $\mathbb N$. So all finite sets are countable. $\mathbb Z$ is countable. Indeed, define a function $f : \mathbb Z\to\mathbb N$ as following:

\[
\left\{\begin{aligned}0&\mapsto0\\n&\mapsto 2n\\-n&\mapsto 2n+1\end{aligned}\right.
\] This is a bijection.

-------------

$\mathbb Q$ is countable as well. We use the Cantor's diagonal argument. This is just to illustrate, to prove this formally, we need something like cartesian product of countable set is countable and so on, perhaps even Schroder-Berstein.


\[
\frac{1}{1}, \frac{2}{1}, \frac{3}{1}, \frac{4}{1},...\]
\[\frac{1}{2}, \frac{3}{2}, \frac{5}{2}, \frac{7}{2},...\]
\[\frac{1}{3}, \frac{2}{3}, \frac{4}{3}, \frac{5}{3},...\]



## Definition of $\mathbb R$ 

In this section we are following Zorich, Mathematical Analysis.

Any set $\mathbb R$ is called a set of real numbers if the following axioms holds:

### Axioms for addition

An operation $+:\mathbb R \times \mathbb R \to \mathbb R$ should satisfy:

1. There is an additive identity called $0$ such that for every $x\in\mathbb R$, $x+0=0+x=x$.
2. For every $x\in\mathbb R$,there exists $-x\in\mathbb R$ such that $x+(-x)=(-x)+x=0$.
3. $+$ is associative: for every $a,b,c\in\mathbb R$, $a+(b+c)=(a+b)+c$.
4. $+$ is commutative: for every $a,b\in\mathbb R$, $a+b=b+a$.

### Axioms for multiplication

An operation $.:\mathbb R\times\mathbb R\to\mathbb R$ should satisfy:

1. there is a multiplicative identity $1\in\mathbb R-\{0\}$ such that for all $x\in\mathbb R$, $x1=1x=x$
2. For every $x\in\mathbb R-\{0\}$, there exists an element $x^{-1}\in\mathbb R$ such that $xx^{-1}=x^{-1}x=1$.
3. multiplication is associative: for any $a,b,c\in\mathbb R$, $a(bc)=(ab)c$.
4. multiplication is commutative: for any $a,b\in\mathbb R$, $ab=ba$.

### Connection between addition and multiplication

Multiplication should distribute with respect to addition: for every $a,b,c\in\mathbb R$,$(a+b)c=ac+bc$.

### Oder axioms

There is $\le$ on $\mathbb R$ such that:

1. $\forall x\in\mathbb R(x\le x)$
2. $\forall x,y\in\mathbb R,(x\le y)\land (y\le x)\implies(x=y)$
3. $\forall x,y,z\in\mathbb R,(x\le y)\land(y\le z)\implies(x\le z)$
4. $\forall x,y\in\mathbb R, (x\le y)\lor (y\le x)$

### Connection between order and addition

For any $x,y,z\in\mathbb R$, $(x\le y)\implies(x+z\le y+z)$.

### Connection between order and multiplication

For any $x,y\in\mathbb R$, $(0\le x)\land (0\le y)\implies(0\le xy)$.

### Axiom of completeness

A set $X\subseteq\mathbb R$ is said to be bounded above if and only if $\exists K\in\mathbb R,\forall x\in X,x\le K$. Such $K$ is called an upper bound. A supremum of $X$ is an upper bound $a$ such that $\forall \epsilon>0\exists b\in X,b>a-\epsilon$.
Let $\emptyset\ne X\subseteq\mathbb R$ be bounded above. $X$ attains a supremum in $\mathbb R$.

A dual notion of bounded above and supremum is bounded below and infimum: A set $X\subseteq\mathbb R$ is said to be bounded above if and only if $\exists K\in\mathbb R,\forall x\in X,x\ge K$. Such $K$ is called an upper bound. A infimum of $X$ is an upper bound $a$ such that $\forall \epsilon>0\exists b\in X,b<a+\epsilon$.

$\mathbb N$ lives in real as $\{0,1,1+1,1+1+1,...\}$. $\mathbb Z$ lives in real as $\mathbb N$ and all the additive inverse of $\mathbb N$. $\mathbb Q$ lives in real multiples of all the multiplicative inverse of $\mathbb Z$.

-----------

#### Consequences of axiom of completeness

<div class="Theorem">
Let $\emptyset\ne X\subseteq\mathbb R$ be bounded below $X$ attains a infimum in $\mathbb R$.
</div>

<div class="Proof">
Let $Y$ be the set $\{-x|x\in X\}$. $Y$ is bounded above: Since $\exists K\in\mathbb R,\forall x\in X, x\ge K.$ Then $-x\le -K$. Hence $-K$ is the upper bound of $Y$. By axiom of completeness, $Y$ has supremum $a$. Then $-a$ is the infimum of $X$: $a$ is upper bound of $Y$, so $-a$ is upper bound of $X$. Fix $\epsilon>0$, $\exists y\in Y,y>a-\epsilon$. Then $-y\in X$ and $-y<-a+\epsilon$.
</div>

--------

<div class="Theorem">[Archimedean Principle]
\[\forall x\in\mathbb R\exists n\in\mathbb Z,n\le x\]
</div>

<div class="Proof">
Otherwise, for some $x$, $\forall n\in\mathbb Z,n>x$. So $\mathbb Z$ is bounded above. Then $\sup \mathbb Z$ exists. Then $\sup \mathbb Z-1$ is not upper bound for $\mathbb Z$. So for some integer $m\in \mathbb Z$, $m>\sup\mathbb Z-1$. Then $m+1$ being integer is $>\sup\mathbb Z$. Contradiction.
</div>

So $\mathbb N$ is bounded.
So $\forall x>0,\exists n\in\mathbb N,\frac{1}{n}<x$. Because $\exists n\in\mathbb Z,n>\frac{1}{x}$, then rearrange.

-----------

<div class="Theorem">[well-ordering of $\mathbb Z$]
Any non-empty bounded below subset $S$ of $\mathbb Z$ has a minimum (i.e. supremum actually $\in S$).
</div>

<div class="Proof">
So $S$ has infimum $s$. Then $s+1$ is not a lower bound, hence for some $m\in S$, $m<s+1$. Then $m-1<s\le m$. Actually $m$ is the minimum of $S$ because otherwise for some $n\in S$ with $n<m$. Then $m>n\ge s>m-1$. So $n>m-1$, this is the same as $n\ge m$. So $m>n\ge m$, i.e $m>m$. Contradiction.
</div>

So nonempty bounded above subset of $\mathbb Z$ has a maximum.

----------

<div class="Theorem">
\[\forall a\in\mathbb R,\forall b\in \mathbb R [(b>a)\implies(\exists r\in\mathbb Q, a<r<b)]\]
</div>

<div class="Proof">
By Archimedean, $\exists n\in\mathbb N,\frac{1}{n}<(b-a)$. So $na < nb-1$. Let $S:=\{m\in\mathbb Z|m<nb\}$. So $S$ is bounded above and nonempty. So $S$ has maximum $m$. $m<nb\le m+1$. The first inequality is from $m\in S$. The second is from $nb-1\in S$, then $nb-1\le m$ then $nb\le m+1$. So $na<nb-1\le (m+1)-1=m<nb$. So $na<m<nb$, divide n, $a<\frac{m}{n}<b$
</div>

----------

<div class="Theorem">
\[\exists! x>0,x^2=2\]
</div>

Existence: take $x$ to be $\sup\{y|y^2\le2\}$ which is nonempty ($1$ is in that) and bounded above by 2. We now prove $x^2$ is indeed $2$: Otherwise $x^2<2$ or $x^2 > 2$.

- if $x^2>2$. Let $\epsilon = \frac{x^2-2}{2x}$. Then $\epsilon>0$ and $(x-\epsilon)^2=x^2-2x\epsilon+\epsilon^2>x^2-2x\epsilon=x^2-2x\frac{x^2}{2x}=2$. Then $x-\epsilon$ is also upper bound but $x$ is the smallest.

- if $x^2<2$. Let $\epsilon =\frac{2-x^2}{2x+1}$. Then $0<\epsilon<1$ ($<1$ because $(x+1)^2>2$ otherwise $x+1\in\{y|y^2\le 2\}$, then $x+1\le x$). So $(x+\epsilon)^2=x^2+2x\epsilon+\epsilon^2<x^2+2x\epsilon+\epsilon=x^2+\epsilon(2x+1)=x^2+\frac{2-x^2}{2x+1}(2x+1)=2$.

Uniqueness is because supremum is unique. Let $S$ be any set, suppose $a,b$ are both supremum, then $a\le b$ and $b\le a$.

--------

### Exercises

Note that we are using *a lot* of properties of addition, multiplication and order of $\mathbb R$ we haven't proved but can be proven easily from the axioms. Try spot them and prove them. For example we used somewhere if $x>0$ then $\frac{1}{x}<y\iff\frac{1}{y}<x$ etc.

-----------

Just from the list of axioms, we can say nothing about the uniqueness or existence of real numbers. There might be many real numbers  or might be none at all. In this section we sketch a proof that real number if exists, is unique up to order-preserving isomorphism. In next section we will actually construct a real number set.

## Uniqueness of $\mathbb R$ 

The above axioms defines what we call a complete ordered fields. We are going to prove any two complete ordered fields are isomorphic.

<div class="Definition">
Let $(k_1,+_1,0_1,\times_1,1_1,\le_1),(k_2,+_2,0_2,\times_2,1_2,\le_2)$ be two complete ordered fields. An order-preserving isomorphism $\phi:k_1\to k_2$ is a bijection such that
\begin{enumerate}
\item $\forall x,y\in k_1,x\le_1y\implies\phi(x)\le_2\phi(y)$
\item $\forall x,y\in k_1,\phi(x+_1 y)=\phi(x)+_2\phi(y)$
\item $\forall x,y\in k_1,\phi(x\times_1 y)=\phi(x)\times_2\phi(y)$
\end{enumerate}
</div>

Let $\mathbb N_i=\{0_i,1_i,1_i+_i1_i,1_i+_i1_i+1_i,\cdots\}$ be the natural numbers inside $k_i$. Let $\mathbb Z_i$ be the corresponding integers inside $k_i$ and $\mathbb Q_i$ be rationals ($\{a\times b^{-1}|a\in\mathbb Z_i,b\in\mathbb Z_i-\{0\}\}$).

To prove the existence of such order-preserving isomorphism, we deploy the following strategy:

1. We construct an isomorphism from $\mathbb N_1$ to $\mathbb N_2$;
2. Then extends it to an isomorphism from $\mathbb Z_1$ to $\mathbb Z_2$;
3. Then extends it to an isomorphism from $\mathbb Q_1$ to $\mathbb Q_2$;
4. Then finally extends it to $k_1\to k_2$.


So let us define $\phi:\mathbb N_1\to\mathbb N_2$ by $\phi(0_1)=0_2$ and $\phi(1_1)=1_2$. Then everything else is forced : $\phi(2_1)=\phi(1_1+_11_1)=\phi(1_1)+_2\phi(1_1)=1_2+_2+1_2=2_2$. Prove by induction we can see in general $\phi(n_1)=n_2$. ($n_i$ is $1_i+_i+1_i\cdots+_i1_i$ $n$ times.)

Then since $(-a)=(-1)\times a$ for all $a\in k_i$. If we require $\phi((-1)_1)=(-1)_2$. Everything is defined from $\mathbb Z_1\to\mathbb Z_2$. Similarly we can extend it again by $\phi(a\times_1 b^{-1})=\phi(a)\times_2\phi(b)^{-1}$. Check that everything defined so far is order preserving and bijection: For example, assume $n_1,m_1,a_1,b_1$ are positive integers in $\mathbb Z_1$, if $n_1\times_1m_1^{-1}\le a_1\times_1b_1^{-1}$ by the order axioms, we can rearrange $n_1\times_1 b_1\le_1 a_1\times_1 m_1$. Then apply $\phi$, $n_2\times_2 b_2\le_2a_2\times_2m_2$. So everything works if you want to check them.

So far everything is easy, but extending it to whole of $k_1\to k_2$ is slightly trickier.

For an arbitrary $r_1\in k_1$. The set $S(r_1):=\phi(\{q\in\mathbb Q_1|q\le r_1\})$ is non-empty. We can easily prove this using the theorem above, namely $\exists q\in\mathbb Q_1,r_1-1<q<r_1$. Then $\phi(q)\in S(r_1)$, also $\exists q\in\mathbb Q_1,q>r_1$ for similar reason, then $\phi(q)$ is an upperbound for $S(r_1)$. So we can define $\phi(r_1):=\sup S(r_1)$. We can check order preserving: if $r_1\le t_1$ then $\{q\in\mathbb Q_1|q\le r_1\}\subseteq\{q\in\mathbb Q_1|q\le t_1\}$ then $S(r_1)\subseteq S(t_1)$. Then $$S(r_1)\subseteq S(t_1)\implies \sup S(r_1)\le_1 \sup S(t_1)\implies\phi(r_1)\le_2\phi(t_2)$$


The first implication is general and requires a little bit of thought: for any two sets $S\subseteq T$, if their supremum exists then $\sup S\le \sup T$. Clearly upperbounds for $T$ are upperbounds for $S$ as well. For the purpose of contradiction say $\sup S > \sup T$, then $\exists s\in S,s>\sup T$. But this $s\in T$, contradiction.

$\phi$ being injective is relatively straightforward. for if $r\ne s$, then either $r<_1 s$ or $s<_1 r$. Assuming without lost of generality $r<_1 s$. Then $r<_1 s-\epsilon$ for some $\epsilon>0$ but small enough (also by archimedean). Then $\phi(s-\epsilon)$ is still upperbound for $S(r)$ but not $S(s)$. So we need to prove subjectivity : for any $r_2\in k_2$, there is $r_1\in k_1$ such that $\phi(r_1)=r_2$. Define $r_1:=\sup \phi^{-1}(\{t_2\in k_2|t_2\le r_2\})$. We need first to show $r_1$ exists. So $\phi^{-1}(\{t_2\in k_2|t_2\le r_2\})$ is nonempty and bounded above. Here is a sketch there are rationals $\alpha_2,\beta_2\in\mathbb Q_2$ such that $\alpha_2<r_2$ and $\beta_2>r_2$. $\phi:\mathbb Q_1\to\mathbb Q_2$ is bijective and order preserving so $\phi^{-1}(\alpha_2)\in\phi^{-1}(\{t_2\in k_2|t_2\le r_2\})$ and $\phi^{-1}(\beta_2)$ is an upperbound. 

### Exercise

1. We also need to check $\phi(r_1)=r_2$.
2. For any two sets $A,B$, write $A+B:=\{a+b|a\in A,b\in B\}$. Then $\sup(A+B)=\sup A+\sup B$. Use this to prove $\phi(s+_1 t)=\phi(s)+_2\phi(t)$.
3. Similarly prove $\phi(s\times_1t)=\phi(s)\times_2\phi(t)$.

This finishes the sketch that every two real numbers is isomorphic.


## Existence of $\mathbb R$ 

<div class="Definition">
Let $A,B$ be a partition of $\mathbb Q$, $(A, B)$ is a Dedekind's cut if the following hold:
\begin{enumerate}
\item $A\ne\mathbb Q$ and nonempty;
\item $\forall y\in A,x\le y\implies x\in A$;
\item $\forall x\in A,\exists y\in A,y>x$ ($A$ has no greatest element).
\end{enumerate}
</div>

Then we define real numbers to be all Dedekind's cut.
Let $(A_1,B_1)$ and $(A_2,B_2)$ be two Dedekinds cut, then define

1. $(A_1,B_1)+(A_2,B_2):=(A_1+A_2,B_1+B_2)$
2. $(A_1,B_1)\times(A_2,B_2):=(\{a_1a_2|a_1\in A_1,a_2\in A_2,\textrm{at least one of } a_1, a_2\textrm{ is nonnegative.}\})$.
3. $(A_1,B_1)\le (A_2,B_2)$ if $A_1$ is a proper subset $A_2$.

The exercise is to prove the axioms. Quite painful, if you are not masochistic, ignore it. The important bit: if $S=\{(A_i,B_i)|i\in I\}$ is a set of dedekind's cut, nonempty and bounded. Then one can prove $\sup S=\left(\bigcup_{i\in I}A_i,\mathbb Q-\bigcup_{i\in I}A_i\right)$.

### Example

1. Let $r$ be in $\mathbb Q$, $(\{t\in\mathbb Q|t<r\},\{t\in\mathbb Q|t\ge r\})$ represents $r$ in $\mathbb R$
2. $(\{x\in\mathbb Q|x^2<2\},\{x\in\mathbb Q|x^2>2\})$ represents $\sqrt2\in\mathbb R$

See @rudin1964principles for more details on Dedekinds cuts.

## $\mathbb R$ is uncoutable

There is a theorem (see @zorich2016mathematical page 63) saying that any $r$ real number can be uniquely represented by $f:\mathbb N\to\{0,1,2,3,4,5,6,7,8,9\}$. For example $\sqrt2$ is represented by $f(0)=1,f(1)=4,f(2)=1,f(3)=4$ etc (i.e the decimal expansion).

Let us prove no bijection $\mathbb R\to\mathbb N$. Suppose otherwise $\mathbb R$ is countable then suppose the bijection $g$ is like the following $n\mapsto a_{n0}a_{n1}a_{n2}a_{n3}...$. We can define a number $b$ whose decimal expansion $b_0b_1b_2b_3...$ such that $b_i\ne a_{ii}$. Then no natural number maps to $b_0b_1b_2...$ because $g(i)$ has $i$th digit different from $b$.

Then no bijection $\mathbb N\to\mathbb R$, so not countable.

## Summary

I think the pièce de résistance in this lecture is not how real numbers are actually constructed, but the axioms themselves. There are many construction of real numbers, but by our sketch of proof, they are essentially the same (up to an order preserving isomorphism). $\mathbb R$ is still not a perfect field in the sense of algebraic closedness (i.e. if all polynomials have a root). But to fix that we go to the construction of $\mathbb C$ (normally as $\frac{\mathbb R[X^2]}{(X^2+1)}$, if you know more algebra), however we are losing ordering property on $\mathbb C$.

Something being unique up to isomorphism and properties being preserved under isomorphism are important in mathematics because then we can forget about the actually details and look at some abstract entities instead or if we don't want to work with some abstract entity we can work with a more concrete realisation. In our case, Dedekind's cut is hard to work with, because of the uniqueness, we don't actually care about Dedekind's cut that much other than that it can be done so real numbers exist, the axioms is all we need.



# Sequences and limits (Lecture 5)

In this lecture, we will discuss sequences and relevant notions. Sequences are of many forms, there can be sequences of real numbers, of functions, of integrals etc. We are going to make sense of notions of limits, convergence etc.

## Sequence

<div class="Definition">
A sequence of real numbers is a function $f:\mathbb N\to \mathbb R$. Often we write $(a_n)_{n\in\mathbb N}$ to mean the sequence with $n$-th element to be $a_n:=f(n)$.
</div>

Often we omit the subscript $n\in\mathbb N$ part. Or we may write $(a_n)_{n=i}^\infty$ to mean the sequence $f(n)=a_{n+i}$, i.e the sequnce $(a_i,a_{i+1},a_{i+2},...)$. Again we usually discard the $_{n=i}^{\infty}$ part if it is clear. For example $\left(\frac{1}{n}\right)$ means $\left(\frac{1}{n}\right)_{n=1}^\infty$, so it is $\left(\frac{1}{1},\frac{1}{2},...\right)$ because dividing by zero is not allowed. Normally exact which index we start with is either clear from context or doesn't really matter.

- So $(n)_{n\in\mathbb N}$ is the sequence $(0,1,2,3,\cdots)$,
- So $(\frac{1}{n+1})_{n\in\mathbb N}$ is the sequence $\left(1,\frac{1}{2},\frac{1}{3},...\right)$,
- etc.

<div class="Definition">
A null sequence $(a_n)$ is a sequence such that 
\[
  \forall \epsilon>0\exists N\in\mathbb N\forall n>N, |a_n|<\epsilon
\]
</div>

Whether $|a_n|<\epsilon$ or $|a_n|\le \epsilon$ doesn't really matter: Say $\forall \epsilon>0\exists N\in\mathbb N\forall n>N, |a_n|\le\epsilon \iff \forall \epsilon>0\exists N\in\mathbb N\forall n>N, |a_n|<\epsilon$. From right to left is trivial. From left to right is as following:
Fix any $\epsilon > 0$. Consider $\frac{\epsilon}{2}$, then take the $N\in\mathbb N$ such that $\forall n>N, |a_n|\le \frac{\epsilon}{2}<\epsilon$.

Because for real number $a$, $||a||=|a|$, we have $(a_n)_{n\in\mathbb N}$ is null if and only if $(|a_n|)_{n\in\mathbb N}$ is null. Here are some examples of null sequences:

- $(a_n)_{n\in\mathbb N}$ such that $\forall n\in\mathbb N,a_n=0$. For any $\epsilon>0$, take $N=0$ will do.
- $(\frac{1}{n})_{n=1}^{\infty}$ is a null sequence. Take $\epsilon>0$, by archemidean, there is some $N\in\mathbb N$ such that $N>\frac{1}{\epsilon}$. Then for any $n>N$, $|\frac{1}{n}|=\frac{1}{n}<\frac{1}{N}<\epsilon$.
- $\left(\frac{(-1)^n}{n}\right)_{n=1}^{\infty}$.
- If $(a_n)_{n\in\mathbb N}$ is null then $(b_n):=(a_n)_{n=i}^{\infty}$ is null for any $i$. For any $\epsilon>0$, there is some $N\in\mathbb N$ such that $\forall n>N,|a_n|<\epsilon$. Then because of $b_n=a_{n+i}$, $\forall n>N+i,|b_n|=|a_{n+i}|<\epsilon$, since $n+i>N+i$ implies $n>N$.

### Cauchy sequence

<div class="Definition">
A sequence $(a_n)$ of real numbers is Cauchy if and only if:
\[\forall \epsilon>0\exists N\in\mathbb N \forall n>N\forall m>N, |a_n - a_m|<\epsilon\]
</div>

<div class="Example">
$(\frac{1}{n^2})$ is Cauchy: fix $\epsilon>0$, there is some $N\in\mathbb N$ such that $2^N>\frac{2}{\epsilon}$ i.e. $2^{-N}<\frac{\epsilon}{2}$. Then
\[\left|\frac{1}{2^n}-\frac{1}{2^m}\right|\le\frac{1}{2^n}+\frac{1}{2^m}\le\frac{1}{2^N}+\frac{1}{2^N}<\epsilon\]
</div>

## Limit

<div class="Definition">
Let $(a_n)$ be a sequence we say $(a_n)$ converges to $a\in\mathbb R$ if $(a_n-a)_{n\in\mathbb N}$ is null. Or equivalently

\[
\forall\epsilon>0\exists N\in\mathbb N\forall n>N,|a_n-a|<\epsilon
\]

We write this as $\lim_{n\to\infty} a_n=a$. If $(a_n)$ doesn't converge to anything in $\mathbb R$, we say that $a_n$ diverges. If $\forall M>0\exists N\in\mathbb N\forall n>N,a_n>M$ we say $(a_n)$ diverges to $\infty$. If $\forall M<0\exists N\in\mathbb N\forall n>N,a_n<M$, we say $(a_n)$ diverges to $-\infty$

</div>


So by definition a null sequence converges to $0$.

<div class="Example">

- $\left(\frac{n}{n+1}\right)$ converges to $1$. Indeed $\left(\frac{1}{n+1}\right)$ is null.
- But $((-1)^n)$ is not convergent. Because say $\lim_{n\to\infty}(-1)^n=x$ for some $x$. Then take $\epsilon=\frac{1}{2}$. If $x\ge 0$ then for $n$ odd, $|x-(-1)^n|=|x+1|\ge 1>\epsilon$. If $x<0$, then for any $n$ even
$|x-(-1)^n|=|x-1|=|-x+1|>1>\epsilon$. So for any $N\in\mathbb N$, there is always some $n>N$ such that $|x-a_n|\ge \epsilon$. $(-1)^n$ neither diverges to $\infty$ nor $-\infty$.
- $(n^2)$ diverges to $\infty$.
- $(-n)$ diverges to $-\infty$.

</div>


### Properties of limit

<div class="Theorem">
A sequence $(a_n)$ if converge, has only one limit.
</div>

<div class="Proof">
Say $\lim a_n=a$ and $\lim a_n=b$. Then for any $\epsilon>0$, there is an $N_1\in\mathbb N$ such that $|a_n-a|<\frac{\epsilon}{2}$ for all $n>N_1$ and there is $N_2\in\mathbb N$ such that $|a_n-b|<\frac{\epsilon}{2}$. Then for all $n>\max(N_1,N_2)$, we have $|b-a|=|b-a_n+a_n-a|\le|b-a_n|+|a_n-a|\le\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$. If $a\ne b$, either $a<b$ or $a>b$ but then $|a-b|>0$, by (some corollary of) Archemedian principle for some $M\in\mathbb N$, $\frac{1}{M}<|a-b|$. Taking $\epsilon=\frac{1}{M}$, we get a contradiction, hence $a=b$.

</div>

<div class="Theorem">
Any convergent sequence $(a_n)$ is bounded, i.e. $\exists (K_1,K_2)\in\mathbb R\forall n\in\mathbb N,K_2<a_n<K_1$.
</div>

<div class="Proof">
Say $\lim a_n=a$. Then there is some $N\in\mathbb N$ such that $\forall n>N,|a_n-a|<1$, i.e. $a-1<a_n<a+1$. Then $\forall n\in\mathbb N, \min\left(a_0,a_1,\cdots,a_{N-1},a-1\right) <a_n<\max\left(a_0,a_1,a_2,\cdots,a_{N-1},a+1\right)$.
</div>

Note the converse is not true. $((-1)^n)$ is bounded but not convergent.


<div class="Theorem">
Let $(a_n)$ and $(b_n)$ be 2 sequences such that $\lim a_n=a$ and $\lim b_n=b$ and $\forall n\in\mathbb N,a_n\le b_n$. Then $a\le b$.
</div>

<div class="Proof">
Say otherwise $a>b$. Let $\epsilon=\frac{a-b}{2}$. Take $N_1,N_2\in\mathbb N$ be the natural numbers such that $\forall n>N_1,|a_n-a|<\epsilon$ and $\forall n>N_2,|b_n-b|<\epsilon$. Then let $N=\max(N_1,N_2)$ then for any $n>N$, $a-a_n\le|a_n-a|<\epsilon$ and $b_n-b\le|b-b_n|<\epsilon$, i.e. $a-\epsilon<a_n$ and $b_n<b+\epsilon$. So $b_n<b+\epsilon=a-\epsilon<a_n$. Contradiction.
</div>

<div class="Theorem">[Sandwich rule]
Let $a\in\mathbb R$ and $(a_n),(b_n)$ and $(c_n)$ be sequences. Suppose $forall n\in\mathbb N, a_n\le b_n\le c_n$ and $(a_n)$ and $(c_n)$ both converge to $a$. Then $(b_n)$ converges to $a$ as well. 
</div>

<div class="Proof">
Fix an $\epsilon > 0$. We can find $N_1,N_2\in\mathbb N$ such that
\[
[(\forall n>N_1), a-\epsilon<a_n]\land[(\forall n>N_2, c_n<a+\epsilon)]
\]

Then for all $n>\max(N_1,N_2)$, we have $a-\epsilon<a_n\le b_n\le c_n<a+\epsilon$. So $|b_n-a|<\epsilon$. Since the abobe argument works for any $\epsilon>0$, we must have $|b_n-a|=0$. Hence $b_n=a$
</div>


This theorem is handy to compute limits.

<div class="Theorem">
Let $\lim a_n=a$ and $\lim b_n=b$. Then

- $\lim (a_n+b_n)=a+b$;
- $\lim (a_n b_n)=ab$;
- provided that $b\ne 0$ and $\forall n\in\mathbb N, b_n\ne 0$, $\lim \left(\frac{a+n}{b_n}\right)=\frac{a}{b}$.
</div>

<div class="Proof">
- Fix $\epsilon > 0$. Choose $N_1$ in the way that $\forall n>N_1,|a_n-a|<\frac{\epsilon}{2}$ and $N_2$ such that $\forall n>N_2, |b_n-b|<\frac{\epsilon}{2}$. Let $N=\max(N_1,N_2)$. So $|(a_n+b_n)-(a+b)|=|(a_n-a)+(b_n-b)|\le|a_n-a|+|b_n-b|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$.
- By one of the previous theorem $(b_n)$ is bounded. Let $K$ be the real number such that $\forall n\in\mathbb N, |b_n|\le K$. Replace $K$ with $\max(K,|a|)$ if necessary, we can assume $|a|\le K$. So we have
\[
|a_nb_n-ab|=|a_nb_n-ab_n+ab_n-a_nb_n|\le|a_n-a||b_n|+|a||b_n-b|\le K(|a_n-a|+|b_n-b|)
\]
So choose $N_1$ such that $\forall n>N_1, |a_n-a|<\frac{\epsilon}{2K}$, choose $N_2$ such that $\forall n>N_2, |b_n-b|<\frac{\epsilon}{2K}$. Let $N=\max(N_1,N_2)$, then $|a_nb_n-ab|<\epsilon$
- We have
\[
|\frac{a_n}{b_n}-\frac{a}{b}|=|\frac{a_n b-ab_n}{b_n b}|\le\frac{|a_n-a||b|+|a||b_n-b|}{b_nb}
\]

Since $b\ne 0$, we can choose $N_1\in\mathbb N$ such that $\forall n>N, |b-b_n|<\frac{|b|}{2}$. Since $|b-b_n|\ge |b|-|b_n|$. Hence we have for all $n>N_1, |b_n|>\frac{1}{2}|b|$. Fix $\epsilon > 0$, Choose N_2 such that for all $n>N_2$, $|a_n-a|<\frac{1}{4}|b|\epsilon$. Choose $N_3$ such that forall $n>N_3$, $|b_n-b|<\frac{1}{4a}b^2\epsilon$.
So forall $n>\max(N_1,N_2,N_3)$, we have $\left|\frac{a_n}{b_n}-\frac{a}{b}\right|<\epsilon$
</div>


### Monotone sequence

<div class="Definition">

- A sequence $(a_n)$ is said to be increasing if $\forall n\in\mathbb N,(a_n\le a_{n+1})$; strictly increasing if $\forall n\in\mathbb N, a_n<a_{n+1}$. Descreasing sequence and strictly decreasing sequences are similarly defined.

- A monotone sequence is either an increasing or decreasing sequence.
</div>

<div class="Example">
- $(n^2)$ is increasing hence monotone.
- $\left(\frac{1}{n}\right)$ is decreasing hence monotone.
- $(a_n)$ where all $a_n=0$ is monotone, because it's is an increasing (also decreasing) sequence.
- $\left((-1)^n \frac{1}{n}\right)$ is not monotone.
</div>

<div class="Theorem">
For an increasing sequence, bounded above $\iff$ convergent
</div>

<div class="Proof">
From left to right is general (doesn't need increasingness). The other direction:

Just view $(a_n)$ as a set: $A:=\sup\{a_n:n\in\mathbb N\}$. Bounded above and nonempty hence $\sup A$ exists, call it $a$. Let $\epsilon>0$, then there is $N\in\mathbb N$ such that $a_N>a-\epsilon$. Since increasing, forall $n>N$, $a_n\ge a_N>a-\epsilon$.
</div>

Here is the dual theorem
<div class="Theorem">
For a decreasing sequence, bounded below $\iff$ convergent.
</div>

The proof is too similar to be worthy talking. Left as an exercise.

#### Examples and exercises

<div class="Example">
Consider $(a_n)$ such that $a_n = \sqrt{a_{n-1}+2}$ and $a_0=\sqrt{2}$.

- Then $(a_n)$ is bounded.
  Indeed $\forall n\in\mathbb N, 0<a_n\le 2$.
  <div class="Proof">
  Positivity is definition: $a_0 > 0$. $\sqrt{}$ is defined to be the postive square root, all we need is $a_{n-1}+2$ nonzero, but if we used induction $a_{n-1}$ by induction hypothesis is postive, adding 2 is (even more) positive. Let's focus on the $\le$ part via induction. $\sqrt{2}\le2$. Assume $a_{n-1}\le2$, then $a_n=\sqrt{a_{n-1}+2}\le\sqrt{2+2}=2$. Here we used $0\le x\le y\implies \sqrt{x}\le\sqrt{y}$. This is from axioms of real numbers: otherwise $\sqrt x>\sqrt y$ then $x > y$.
  </div>
- $(a_n)$ is increasing.
  <div class="Proof">
  Let $n\in\mathbb N$, then $\sqrt{a_n+2}\ge a_n\iff a_n+2\ge a_n^2\\ \iff a_n+2-a_n^2\ge0\iff (2-a_n)(a_n+1)\ge0$. But $a_n+1>0$ so $2-a_n\ge 0$.
  </div>
- $(a_n)$ converges to 2.
  <div class="Proof">
  So by the previous theorem, $(a_n)$ is convergent, say $\lim a_n=x$, by one of the previous theorem: since $a_n^2=a_{n-1}+2$ we have $x^2=x+2$ so $x=2$ or $x=-1$. But since all $a_n>0$, $x\ge 0$. So $x=2$.
  </div>
</div>


<div class="Example">[@zorich2016mathematical P88]

If $q>1$, then $\lim_{n\to\infty}\frac{n}{q^n}=0$.
<div class="Proof">
Let us denote $x_n=\frac{n}{q_n}$, then $x_{n+1}=\frac{n+1}{nq}x_n$. $\lim \frac{n+1}{nq}=\lim \left(\frac{n+1}{n}\times\frac{1}{q}\right)=\frac{1}{q}<1$. Then there is some $N\in\mathbb N$, such that forall $n>N,\frac{n+1}{n}q<1$. So $x_{n+1}<x_n$ for $n>N$. So the sequence $(x_n)_{n=N}^\infty$ is decreasing so a limit exists, say $x=\lim x_n$. Then $x=\lim (x_{n+1})=\lim \left(\frac{n+1}{nq} x_n\right)=\frac{1}{q}x$. So $x=0$.
</div>

From this we have some interesting corollaries

<div class="Corollary">
\[\lim_{n\to\infty}\sqrt[n]{n}=1\]
</div>

<div class="Proof">
For any $\epsilon > 0$, there is an $N\in\mathbb N$ such that $1\le n<(1+\epsilon)^n\times$ for all $n>N$. Here we use $\lim \frac{n}{(1+\epsilon)^n}=0$. So for some $N\in\mathbb N$, we have $n>N\implies \frac{n}{(1+\epsilon)^n}<1$. So for $n>N$, we have $1\le \sqrt[n]{n}<1+\epsilon$.
</div>

<div class="Corollary">
For any $a>0$, \[\lim_{n\to\infty} \sqrt[n]{a}=1\]
</div>

<div class="Proof">
Let's work with $a\ge 1$ first. For any $\epsilon>0$, there is $N\in\mathbb N$ such that $1\le a<(1+\epsilon)^n$ for all $n>N$. Then $1\le \sqrt[n]{a}<1+\epsilon$.

For $0<a<1$, we have $1<\frac{1}{a}$. So $\lim \sqrt[n]{a}=\lim \frac{1}{\sqrt[n]{\frac{1}{a}}}=\frac{1}{\lim \sqrt[n]{\frac{1}{a}}}=1$
</div>
</div>

### Limits and Cauchy sequences

<div class="Theorem">
Every convergent sequence is Cauchy.
</div>

<div class="Proof">
Let $(a_n)$ be a convergent sequence converging to $a$.
Take any $\epsilon>0$, find $N\in\mathbb N$ such that $|a_n-a|<\frac{\epsilon}{4}$ forall $n>N$. Then for $n>N+1$ and $m>N+1$, \[|a_n - a_m|=|(a_n-a_N)+(a_N-a_m)|\le|a_n-a_{N+1}|+|a_m-a_{N+1}|\\ \le|a_n-a|+|a-a_{N+1}|+|a_m-a|+|a-a_{N+1}|=|a_n-a|+|a_m-a|+2|a-a_{N+1}|<\epsilon\]

This conclude the proof by picking the $N+1$ to be the natural number corresponding to $\epsilon$: $\forall m,n>N+1$ we have $|a_n-a_m|<\epsilon$.
</div>

<div class="Theorem">[Nested interval]
Let $(a_n)$ and $(b_n)$ be real sequences such that:

1. $\forall n\in\mathbb N,b_n>a_n$;
2. $\forall n\in\mathbb N, [a_{n+1},b_{n+1}]\subseteq [a_n, b_n]$;
3. $\lim_{n\to\infty}(b_n-a_n)=0$.
Then $\bigcup_{n\in\mathbb N}[a_n,b_n]$ is nonempty and contains one and only one point.
</div>

<div class="Proof">
From 2, $a_{n+1}\ge a_n\ge a_1$ and $b_{n+1}\le b_n \le b_1$. So $(a_n)$ is increasing and bounded above by $b_1$ so convergent say $\lim a_n = a$; $(b_n)$ is decreasing and bounded below by $a_1$, so convergent say $\lim b_n=b$. Then $a\le b$. But $b-a=\lim(b_n-a_n)=0$ So $a=b$. Since $a\ge a_n\forall n$ and $a=b \le b_n\forall n$, we have $a\in\bigcup_{n\in\mathbb N}[a_n,b_n]$. So nonempty.
Why unique? otherwise say $x\ne y\in\bigcup_{n\in\mathbb N}[a_n,b_n]$. Wlog $y> x$, Then for any $n\in\mathbb N, x,y\in[a_n,b_n]$. Then $b_n-a_n\ge y-x > 0$. Contradicting 3.
</div>

<div class="Theorem">
Every real Cauchy sequence is convergent.
</div>

<div class="Proof">
Say $(x_n)$ is a cauchy sequence. For $n\in\mathbb N$, let us set $a_n:=\inf\{x_k:k\ge n\}$ and $b_n:=\sup\{x_k|k\ge n\}$. So we have $a_n\le a_{n+1}\le b_{n+1}\le b_n$. Let's show $\lim (b_n-a_n)=0$: We need the fact that $\sup (A-B)=\sup A - \inf B$ for any set $A,B$ where $\sup,\inf$ exists and $A-B:=\{a-b|a\in A, b\in B\}$. So $\{x_k:k\ge n\}-\{x_k:k\ge n\}=\{x_i-x_j|i\ge n, k\ge n\}$. So $b_n-a_n=\sup\{x_i-x_j|i\ge n, k\ge n\}$. But $(x_n)$ is Cauchy. So for any $\epsilon >0$ there is $N\in\mathbb N$ such that $\forall i,j>N$ we have $x_i-x_j\le|x_i-x_j|<\epsilon$. So $b_N - a_N < \epsilon$. Since forall $n>N$ we have $b_N\le b_n$ and $a_n\le a_N$, $b_n - a_n < \epsilon$. This finishes the proof.

So by the nested interval principle, there is an $A\in\bigcup_{n\in\mathbb N}[a_n, b_n]$. Since $a_n\le A\le b_n$ for all $n\in\mathbb N$ and for any $k\ge n$ we have $a_n=\inf\{x_j:j\ge n\}\le x_k\le \sup\{x_j:j\ge n\}=b_k$. We must have $|A-x_k|\le b_n-a_n$. By the above argument $A$ is the limit of $(x_n)$: fix $\epsilon > 0$, for some $N\in\mathbb N$, for all $k\ge n>N$, we have $b_n-a_n<\epsilon$ so $|A-x_k|<\epsilon$ forall $k\ge n>N$.
</div>

Combine these we get

<div class="Theorem">[Cauchy criterion for convergence]
A real number sequence is convergent if and only if it is Cauchy.
</div>

Since the prove relies on the completeness axioms, the requirement of being real cannot be dropped. Indeed Cauchy sequence in $\mathbb Q$ may not converge in $\mathbb Q$.

<div class="Example">
Let $(a_n)\in\mathbb Q$ be defined as the decimal approximation of $\sqrt 2\in\mathbb R$. So $a_0 = 1$,
$a_1=\frac{7}{5}(=1.4)$, $a_2=\frac{141}{100}(=1.41)$, $a_3=\frac{707}{500}(=1.414)$ and so on. Then the sequence is Cauchy, but the limit is $\sqrt{2}$ which is not in $\mathbb Q$ but in $\mathbb R$.
</div>

## $e$

The knowledge in this part is not that much, but $e$ is too important, so we list it as a separate part. We first begin by the Bernoulli inequality:

<div class="Theorem">[Bernoulli's inequality]
For any $n\in\mathbb N$ and $\alpha>-1$
\[
(1+\alpha)^n\ge 1 + n\alpha
\]
</div>

<div class="Proof">
True for $n=1$. Suppose $(1+\alpha)^n\ge 1 + n\alpha$, then:
\[
(1+\alpha)^{n+1}=(1+\alpha)(1+\alpha)^n\ge(1+\alpha)(1+n\alpha)=1+(n+1)\alpha+n\alpha^2\\ \ge 1+(n+1)\alpha
\]
</div>

We then consider the sequence $(y_n)_{n=2}^{\infty}$ where $y_n=\left(1+\frac{1}{n}\right)^{n+1}$.

<div class="Theorem">
$(y_n)$ is decreasing.
</div>

<div class="Proof">
\[\frac{y_{n-1}}{y_n}=\frac{\left(1+\frac{1}{n-1}\right)^n}{\left(1+\frac{1}{n}\right)^{n+1}}=\frac{n^{2n}}{(n^2-1)^n}\times\frac{n}{n+1}=\left(1+\frac{1}{n^2-1}\right)^n\frac{n}{n+1}\ge\\\left(1+\frac{n}{n^2-1}\right)\frac{n}{n+1}>\left(1+\frac{1}{n}\right)\frac{n}{n+1}=1\]
The last $>$ is because $\frac{n}{n^2-1}-\frac{1}{n}=\frac{1}{n(n^2-1)}>0$ forall $n>2$.
</div>

<div class="Theorem">
$\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$ exists.
</div>

<div class="Proof">
\[\lim \left(1+\frac{1}{n}\right)^n=\lim\left(1+\frac{1}{n}\right)^{n+1}\left(1+\frac{1}{n}\right)^{-1}\\=\lim \left(1+\frac{1}{n}\right)^{n+1}\times \lim\frac{1}{1+\frac{1}{n}}=\lim \left(1+\frac{1}{n}\right)^{n+1} \]
</div>

<div class="Definition">
\[e:=\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n\]
</div>

## Subsequences

<div class="Definition">
Let $(x_n)_{n\in\mathbb N}$ be a sequence and $(n_k)_{k\in\mathbb N}$ be a seuqnce of natural numbers. We use $(x_{n_k})_{k\in\mathbb N}$ to denote the sequence $(x_{n_0},x_{n_1},...)$, we call this a subsequence of $(x_n)_{n\in\mathbb N}$.
</div>

<div class="Theorem">
Every infinite sequence $(x_{n})\in\mathbb{R}$ has a monotone subsequence.
</div>
<div class="Proof">
Let us call a positive integer $n$ a "peak of the sequence" if $n<m$ implies $x_{n}>x_{m}$. Suppose first that the sequence has infinitely many peaks, $n_0<n_{1}<n_{2}<n_{3}<\dots <n_{j}<\dots$. Then the subsequence $(x_{n_{j}})$ corresponding to these peaks is monotonically decreasing. So suppose now that there are only finitely many peaks, let $N$ be the last peak and $n_0=N+1$. Then $n_0$ is not a peak, since $N<n_{0}$, which implies the existence of $n_1$ with $n_{0}<n_{1}$ and $x_{n_{0}}\leq x_{n_{1}}$. Again, $n_{1}>N$ is not a peak, hence there is an $n_{2}$ where $n_{1}<n_{2}$ with $x_{n_{1}}\le x_{n_{2}}$. Repeating this process leads to an infinite increasing subsequence  $x_{n_{0}}\leq x_{n_{1}}\leq x_{n_{2}}\leq ...$ as desired.

</div>

<div class="Theorem">[Bolzano-Weierstrass]
Every bounded sequence of real numbers has a convergent subsequence.
</div>
<div class="Proof">
Let $E$ be the set ${x_n|n\in\mathbb N}$. If $E$ is finite then there must be some $x\in E$ such that there is $n_0 < n_1 < n_2<...$ such that $x_{n_1}=x_{n_2}=...=x$. So $(x_{n_k})_{k\in\mathbb N}$ is constant hence converges.

If $E$ is infinite, we can use the theorem above to construct a monotone sequence which must converge by previous theorems.
</div>


## Summary

In this lecture, we studied (not in this order)

- What a sequence is, what a monotone sequence is and what a Cauchy sequence is;
- What the limit of a sequence is
- In $\mathbb R$, convergent sequence $\iff$ Cauchy sequence
- We defined $e$



# Series ($\sum$) and mode of convergence (Lecture 6)

<div class="Definition">
Let $(a_n)$ be a sequence of real numbers. Let $s_n=\sum_{i=0}^n a_k$. We say the series $\sum_{i=0}^\infty a_i$ converges if and only if $(s_n)$ converges. If the series doesn't converge we call it a divergent series.
</div>

<div class="Example">
- Recall from high school the geometric progression
\[1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+...\]
The partial sum $s_n=1+\frac{1}{2}+...+\frac{1}{2^n}$ is given by $2-\frac{1}{2^n}$. From previous lecture, this is a convergent series, converging to 0.

- Consider the series $\sum_{i=1}^\infty \frac{1}{i}$. This is a divergent series. Because consider the partial sum $s_n=1+\frac{1}{2}+\frac{1}{3}+...+\frac{1}{n}$. Then $s_{2n}-s_n=\frac{1}{n+1}+...+\frac{1}{n+n}>n\times\frac{1}{2n}=\frac{1}{2}$. So the partial sums don't form a cauchy sequence hence is not convergent. This series is called the harmonic series.
</div>

<div class="Theorem">
If the series $\sum_{n=0}^\infty a_n$ converges then $\lim_{n\to\infty} a_n = 0$
</div>
<div class="Proof">
Let $s_n:=\sum_{i=0}^n a_i$ be partial sums. By definition $(s_n)$ converges, say to $s$. Then $\lim s_{n+1}=s$ Hence $\lim (s_{n+1}-s_n)=0$. Then $\lim a_{n+1}=0$. Then $\lim a_n=0$
</div>

<div class="Theorem">
If the series $\sum_{n=0}^\infty a_n$ converges and $s_n:=\sum_{i=0}^\infty a_i$. Then $s_{2n}-s_n$ converges to $0$.
</div>
<div class="Proof">
Trivial because $\lim s_{2n}=\lim s_n$. Just think about it.
</div>

<div class="Example">
- $1+1+1+1+1+...$ cannot converge because $s_{2n}=2n$ and $s_n=n$. So $s_{2n}-s_n=n$ doesn't converge.
- $\sum_{i=0}^\infty (-1)^i$ cannot converge because $s_{2n}=1$ and $s_n=(-1)^n$. So $(s_{2n}-s_n)$ is the sequence $(0,2,0,2,0,2,...)$. This visibly does not converge.
</div>

<div class="Theorem">[Cauchy's criterion for convergence of series]
$\sum_{n=0}^\infty a_n$ converges if and only if for every $\epsilon>0$ there exists $N\in\mathbb N$ such that for all $m\ge n>N$ we have $|a_n+...+a_m|<\epsilon$.
</div>
<div class="Proof">
Let $s_n:=\sum_{i=0}ˆn a_i$ be partial sums. Then $\sum_{n=0}^\infty a_n$ converges $\iff (s_n)$ converges $\iff (s_n)$ Cauchy $\iff \forall \epsilon>0\exists N\in\mathbb N\forall m\ge n>N, |s_n - s_m|<\epsilon$. But $|s_n - s_m| = |a_n+...+a_m|$. 
</div>

<div class="Definition">
The series $\sum_{i=0}^\infty a_i$ is said to be absolutely convergent if the series $\sum_{i=0}^\infty |a_i|$ is convergent.
</div>

<div class="Theorem">
If the series $\sum_{i=0}^\infty a_i$ is absolutely convergent, then $\sum_{i=0}^\infty a_i$ is convergent
</div>

<div class="Proof">
Since $|a_n+...+a_m|\le |a_n| + ... + |a_m|$, cauchy's creterion, the series converges.
</proof>

<div class="Theorem">[Criterion for convergence of series of series of nonnegative terms]
A series whose terms are nonnegative converges if and only if the sequence of partial sums is bounded above.
</div>

<div class="Proof">
This is monotone convergence theorem: since the terms are non-negative, the sequence of partial sums are non-decreasing, so bounded above $\iff$ convergent.
</div>

## Comparison test

<div class="Theorem">[Comparison theorem]
Let $\sum_{i=0}^\infty a_i$ and $\sum_{i=0}^\infty b_i$ with non-negative terms. If there exists $N\in\mathbb N$ such that for all $n>N$, $a_n\le b_n$. Then if $\sum_{i=0}^\infty b_i$ is convergent, (say to $B$), so is $\sum_{i=0}^\infty a_i$.
</div>

<div class="Proof">
  Let $s_n$ and $s'_n$ be partial sums for series $\sum_{i=0}^\infty a_i$ and $\sum_{i=0}^\infty b_i$. Then $\forall n\in \mathbb N, s_n\le s'_n$. $s_n$ is increasing because of non-negativeness of terms and bounded above by $s_n'\le B$. So $s_n$ is convergent.
</div>

Note that it doesn't really matter whether $s_n\le s_n'$. We can proceed the proof almost in the exact same manner as long as we know for some $K>0$ and $\forall n\in\mathbb N,a_n\le Kb_n$. Because $s_n\le Ks_n'$. Also a series' convergence is unaffected if we just insert some finite number of terms or delete some finite number of terms.

<div class="Theorem">
Let $(a_n)$ and $(b_n)$ be two sequences of postive numbers. Assume $\lim \frac{a_n}{b_n}=L>0$ then $\sum_{i=0}^\infty a_i$ converges if and only if $\sum_{i=0}^\infty b_i$ converges
</div>

<div class="Proof">
So by definition of limit. Take $\epsilon=\frac{L}{2}>0$, we have for some $N\in\mathbb N$ and any $n>N$, we have $|\frac{a_n}{b_n}-L|<\frac{L}{2}$. We have
\[
  \frac{1}{2}L <\frac{a_n}{b_n} < \frac{3}{2}L
\].
This is true by the above remark.
</div>

<div class="Corollary">[The Wierstrass M-test for absolute convergence]
Let $\sum_{i=0}^\infty a_i$ and $\sum_{i=0}^\infty b_i$ be two series. If for some $N\in\mathbb N$ and for all $n>N,|a_n|\le b_n$. Then convergence of $\sum_{i=0}^\infty b_i$ implies absolute convergence of $\sum_{i=0}^\infty a_i$.
</div>

<div class="Example">
- $\sum_{i=0}^\infty\frac{2n}{n^2+1}$ is divergent. Because \[\frac{\frac{2n}{n^2+1}}{\frac{1}{n}}=\frac{2n^2}{n^2+1}\\\lim=2\]. If the series is convergent then $\sum \frac{1}{i}$ is convergent which is not.
- $\sum_{i=1}^\infty \frac{1}{n^2}$ is convergent because \[
  \frac{1}{n(n+1)}<\frac{1}{n^2}<\frac{1}{n(n-1)}\] $\frac1{k+1}=\frac1k-\frac1{k+1}$. So the partial sum for $\sum\frac1{n(n+1)}$ is $1-\frac 1{n+1}$, hence convergent convergent. Similarly for $\sum_{i=2}^\infty\frac 1{n(n-1)}$ is convergent. So $\sum_{i=2}^\infty\frac 1{n^2}$ is convergent. So $\sum_{i=1}^\infty\frac1{n^2}$ is convergent.
- The series $\sum_{i=1}^\infty\frac{n}{2n^3+2}$ is convergent because $\lim\frac{\frac{n}{2n^3+2}}{\frac{1}{n^2}}=\frac{1}{2}$ and $\sum\frac{1}{n^2}$ is convergent.
</div>

## Root test

<div class="Definition">[limit superior and limit inferior]
The _limit superior_ and _limit inferior_ of a sequence $(x_n)$ is defined as \[\limsup_{n\to\infty}x_n:=\lim_{n\to\infty}\sup\{x_k|k\ge n\}\\\liminf_{n\to\infty}x_n:=\lim_{n\to\infty}\inf\{x_k|k\ge n\}\] provided the limits exist otherwise we say \[\limsup_{n\to\infty} {x_n}:=\infty\\\liminf_{n\to\infty} {x_n}:=-\infty\].
</div>

The theory of _limit superior_ and _limit inferior_ can be further developed together with subsequences for example see @zorich2016mathematical.
In short, $\limsup$ is the $sup$ of all the limits of subsequence and $\liminf$ is the $\inf$ of all the limits of subsequence etc.

</div>

<div class="Theorem">[Root test or Cauchy's test] Let $(a_n)$ be a sequence of number and let $\alpha:=\limsup\sqrt[n] {|a_n|}$. Then
- if $\alpha < 1$ then the series converges absolutely;
- if $\alpha > 1$ then the series diverges;
- if $\alpha = 1$ no conclusion can be made: there are both convergent and divergent series with $\alpha=1$.
</div>

<div class="Proof">
- So for some $\alpha<q<1$ and $N\in\mathbb N$ such that we have $\forall n>N, \sqrt[n]{|a_n|}<q$ i.e. $|a_n|<q^n$. Comparing to $\sum_{i=N+1}^\infty q^i$ we conclude $\sum_{i=N+1}^\infty a_n$. So $\sum_{i=0}^\infty a_n$ converges as well.
- So for some $N\in \mathbb N$ we have $\forall n>N, \sqrt[n]{|a_n|}>1$. So $\lim a_n\ne0$, thus diverges.
- Consider $\sum\frac1n$ and $\sum\frac1{n^2}$.
</div>

<div class="Example">
Let us see for what value would the following converge:\[\sum_{i=1}^\infty\left(2+\left(-1\right)^n\right)^nx^n\]
Note that the sequence $\left(\sqrt[n]{\left|\left(2+\left(-1\right)^n\right)^nx^n\right|}\right)$ does not converge for any $x\ne 0$. But $\sup\left\{\sqrt[k]{\left|\left(2+\left(-1\right)^k\right)^kx^k\right|}|k\ge n\right\}=3|x|$ so for $|x|<\frac13$, the sequence converges (absolutely) and for $|x|>\frac13$ the sequence diverges. For $x=\frac13$, the test tells us nothing so we have to work harder: $\left(2+\left(-1\right)^n\right)^n\left(\frac13\right)^n=1$ for $n$ even so the squence does not tends to $0$ hence the series cannot converge for $|x|=1/3$.
</div>

## d’Alembert’s test or ratio test

<div class="Theorem">[ratio test or d'Alembert's test]
Let $(a_n)$ be a sequence of number and let $\alpha:=\lim\left|\frac{a_{n+1}}{a_n}\right|$. Then
- if $\alpha < 1$ then the series converges absolutely;
- if $\alpha > 1$ then the series diverges;
- if $\alpha = 1$ no conclusion can be made: there are both convergent and divergent series with $\alpha=1$.
</div>

<div class="Proof">
- So there is some $\alpha<q<1$ and $N\in\mathbb N$ such that for all $n>N,\left|\frac{a_{n+1}}{a_n}\right|<q$. Since by changing the series with only finitely many terms the series still converge. We have can assume $\left|\frac{a_{n+1}}{a_n}\right|<q$ for all $n\in\mathbb N$. Then \[
  \left|\frac{a_{n+1}}{a_1}\right|=\left|\frac{a_{n+1}}{a_n}\right|\left|\frac{a_n}{a_{n-1}}\right|...\left|\frac{a_2}{a_1}\right|<q^n
\].
So $|a_n|<|a_1|q^n$ with $q<1$. By comparison test, since $\sum|a_1|q^n=|a_1|\sum q^n$ converges, so does $\sum |a_n|$.
- If $\alpha>1$. Then from some $N\in\mathbb N$ onwards $\left|\frac{a_{n+1}}{a_n}\right|>1$. Then $|a_n|$ is stricly increasing from $N$ onwards, so $\lim a_n$ cannot be $0$. Hence the series must diverge.
- Consider $\sum\frac1n$ and $\sum\frac1{n^2}$.
</div>

<div class="Example">
Let us work out for what $x$ does the following converges:
\[\sum_{n=1}^\infty\frac1{n!}x^n\].

- For $x=0$ the series is $\sum 0$ is $0$ converges.
- For $x\ne 0$, $\left|\frac{a_{n+1}}{a_n}\right|=\frac{|x|}{n+1}$ which converges for any $x$. So the series converge everywhere.
- In fact this is $e^x$. But this is hard. Maybe we will prove this after taylor series.
- Let's just work with $x=1$ for now: $\sum_{n=1}^\infty\frac1{n!}=e$ because Let us write $e_n:=\left(1+\frac1n\right)^n$ and $s_n$ be the n-th partial sum of the series. Then \[
e_n=1+{n\choose1}\frac1n+{n\choose 2}\frac{1}{n^2}+\cdots+\frac1{n^n}=1+\frac{n}{1!}\frac1n+\frac{n(n-1)}{2!}\frac1{n^2}+\cdots+\frac1{n^n}\\
=1+1+\frac1{2!}\left(1-\frac1n\right)+\cdots+\frac1{k!}\left(1-\frac1n\right)\left(1-\frac2n\right)\cdots\left(1-\frac{k-1}n\right)+\cdots\frac1{n!}\left(1-\frac1n\right)\cdots\left(1-\frac{n-1}n\right)
\]
This is because ${n\choose k}\frac1{n^k}=\frac{n(n-1)\cdots(n-k+1)}{k!}\frac1{n^k}=\frac1{k!}\frac nn\frac{n-1}n\frac{n-2}n\cdots\frac{n-k+1}n$. Then $e_n<s_n$. For any fixed $k$ and then all $n>k$
\[1+1+\frac1{2!}\left(1-\frac1n\right)+\cdots+\frac1{k!}\left(1-\frac1n\right)\left(1-\frac2n\right)\cdots\left(1-\frac{k-1}n\right)<e_n\]. As $n\to\infty$, lhs tends $s_k$. So $s_k\le e$ for any $k\in\mathbb N$. So $e_n< s_n\le e$. By sandwich theorem $\lim s_n=e$.
</div>

## $\sum_{n=1}^\infty\frac1{n^p}$

I think the following weird theorem is proved by Cauchy but I might be wrong.

<div class="Theorem">
If $a_1\ge a_2\ge a_3\ge...\ge0$ then $\sum_{i=1}^\infty a_i$ converges if and only if $\sum_{i=1}^\infty2^ia_{2^i}=a_1+2a_2+4a_4+8a_8+...$ converges.
</div>

<div class="Proof">
- Left to right trivial.
- Right to left:
Since \[a_2\le a_2\le a_1\\ 2a_4\le a_3+a_4\le2a_2\\4a_8\le a_5+a_6+a_7+a_8\le4a_4\\...\\2^na_{2^{n+1}}\le a_{2^n+1}+\cdots+a_{2^{n+1}}\le2^na_{2^n}\].
If we write $A_k=a_1+\cdots+a_k$ and $S_k=a_1+2a_2+\cdots+2^na_{2^n}$ and add the above inequalies together, we get $\frac12\left(S_{n+1}-a_1\right)\le A_{2^{n+1}}-a_1\le S_n$. This shows that either both $A_k$ and $S_k$ are bounded or neither is bounded. Since $A_k$ and $S_k$ are both increasing, they either both converge or both diverge.
</div>

The importance of above theorem lies in the following corollary.

<div class="Theorem">
$\sum_{n=1}^\infty\frac1{n^p}$ converges for $p>1$ and diverges for $p\le1$.
</div>

<div class="Proof">
- For $p\ge0$, $\sum_{n=1}^\infty\frac1{n^p}$ converges if and only if $\sum_{n=1}^\infty2^n\frac1{\left(2^n\right)^p}$ converges. The latter equals $\sum_{n=0}^\infty\left(2^{1-p}\right)^n$ which converges if and only if $2^{1-p}<1$ if and only if $p < 1$.
- For $p<0$ the series obiously diverges: just look at it $\sum n^q$ with $q=-p\ge 0$.
</div>
$\sum_{n=1}^\infty\frac1{n^p}$ is important because we often use it in comparison test to study convergence of other sequences.

## Summary

In this lecture we studied in details about:

- what a series is and what it means to say it converges;
- some examples of convergent and divergent series, in particular $\sum\frac1{n^p}$;
- some test we can use to see if a series is convergent;
- we also briefly mentioned $e$.

# Riemann Rearrangement Theorem and Power Series (Lecture 7)

## Alternating seuqnece

Last time we mainly talked about absolute convergent. This may not be enough. Recall the harmonic series $\sum\frac1n$ diverges. We are going to focus on series like $\sum(-1)^n\frac1n$ in this lecture. This series converge but not absolutely.

<div class="Theorem">[Alternating series test]
Let $(a_n)$ be a sequence of positive numbers such that

1. $\forall n\in\mathbb N, a_n\ge a_{n+1}$
2. $\lim a_n=0$

Then the series
\[\sum_{n=1}^\infty(-1)^{n-1}a_n=a_1-a_2+a_3-a_4+\cdots
\] is convergent
</div>

<div class="Proof">
Denote by
\[s_n := a_1-a_2+a_3+\cdots+(-1)^{n-1}a_n\] the $n$-th partial sum of the alternating series. Set
\[t_n := s_{2n}=a_1-a_2+a_3+\cdots+a_{2n-1}-a_{2n}\\
u_n:=s_{2n-1}=a_1-a_2+a_3+\cdots+a_{2n-1}
\]

So $t_{n+1}-t_n=s_{2n+2}-s_{2n}=a_{2n+1}-a_{2n+2}\ge0$ and
$u_{n+1}-u_n=-a_{2n}+a_{2n+1}\le0$. Also $u_n-t_n=a_{2n}\ge 0$. So we have $t_n$ increasing, $u_n$ decreasing and $u_n\ge t_n$. Then by monotone convergence theorem $t_n$ converges to some $t$ and $u_n$ converges to some $u$. Also $u-t=\lim(u_n-t_n)=\lim a_{2n}=0$. So $t=u$. This is saying $\lim s_{2n}=\lim s_{2n+1}=t$. We claim that $\lim s_n=t$.
For any $\epsilon>0$, there is $N\in\mathbb N$ such that $\forall k>N$, $|s_{2k}-t|<\epsilon$ and $|s_{2k+1}-t|<\epsilon$. So for all $n>2N+1$, if $n$ is even then $n=2k$ with $k>N$ otherwise $n$ is odd so $n=2k+1$ with $k>N$.
</div>

I would like to point out that knowing the subsequence of even indices and subsequence of odd indices both converge to the same real number though is sufficient for the convergence of real number sequence. (This is essentially because there is not a lot of ways of getting larger and larger in natural number). But this requires reasoning. It is not generally true that the whole sequence converges just because we choose one (or several) ways of how $n$ gets large. This is particularly true when we are dealing with complex functions.

<div class="Example">
The manifested example is the alternating harmonic sequence:
\[1-\frac12+\frac13-\frac14+\cdots\]
Sometimes just to emphasise the series converges but not absolutely we say it converges conditionally. The terminology will be more clear once we know rearranging series.
</div>

<div class="Example">
The series $\sum_{n=1}^\infty\frac{x^n}n$ converges if and only if $x\in[-1,1)$

- If $x>0$ then $\lim\frac{\frac{x^{n+1}}{n+1}}{\frac{x^n}{n}}=\lim\frac{xn}{n+1}=x$. So by ratio test, if $x < 1$, the series converges. If $x>1$, the series diverges. If $x=1$, it is the harmonic series which diverges. For $x<0$, we write $y=-x>0$. The series is $\sum\frac{(-1)^ny^n}n$. For $y\le1$ we have $\frac{y^n}n>\frac{y^{n+1}}{n+1}$ and $\lim \frac{y^n}n=0$. So by previous theorem we know that the series converges. On the other hand for $y>1$ then $\lim \frac {y^n} n$ is not zero. So the sequence diverges.
</div>

## Rearrange series

<div class="Example">[Illustrative example]
Let us work with $1-\frac12+\frac13-\frac14+\cdots=:\sum_{i=1}^\infty(-1)^{n-1}\frac1n$. Note we are *fixing* the order of summation, namely we are considering the limit of partial sum $s_n:=\sum_{i=1}^n(-1)^{i-1}\frac1i$ converging to some $s$. But what if we are using a different order of summation:
\[1-\frac12-\frac14+\frac13-\frac16-\frac18+\frac15-\frac1{10}-\frac1{12}+\cdots\]. If we denote the partial sum as $t_n$. Then

\[t_{3n}=\left(1-\frac12-\frac14\right)+\left(\frac13-\frac16-\frac18\right)+\cdots+\left(\frac1{2n-1}-\frac1{4n-2}-\frac1{4n}\right)\\
=\frac12\left(1-\frac12\right)+\frac12\left(\frac13-\frac14\right)+\cdots+\frac12\left(\frac1{2n-1}-\frac1{2n}\right)=\frac12s_{2n}\].
So $t_{3n+1}=t_{3n}+\frac1{2n+1}$ and $t_{3n+1}=t_{3n+1}-\frac1{4n+2}$. So $\lim t_{3n}=\frac12s$ and $\lim t_{3n+1}=\lim t_{3n}+\lim\frac1{2n+1}=\frac12s$ and $\lim t_{3n+2}=\lim t_{3n+1}-\frac1{4n+2}=\frac12s$. With some argument like the one in last section we know $\lim t_n=\frac12 s$.

So just by rearranging the series, we make the result smaller by half. Similarly we can by rearranging sequence to make the series diverges as well. This is why we call it conditional convergence.
</div>

Let us now work with more generality:
<div class="Definition">
We say $\sum_{n=0}^\infty b_n$ is a rearrangement of $\sum_{n=0}^\infty a_n$ if there is a bijection $\pi:\mathbb N\to\mathbb N$ such that $b_n=a_{\pi(n)}$ for all $n\in\mathbb N$. Of course if our series starts with $i$, then rearrangement is a bijction $\pi:\{i,i+1,\cdots,\}\to\{i,i+1,\cdots\}$ such that $b_n=a_{\pi(n)}$ for all $n\ge i$
</div>

<div class="Theorem">[Rearrangement Theorem I] Let $\sum a_n$ be an *absolute* convergent series converging to $s$. Then all the rearrangement also converge to $s$.
</div>

<div class="Proof">

- First, we restrict our attention to the case where $a_n\ge 0$ for all $n$. Suppose $b_1+b_2+\cdots$ is a rearrangement of $a_1+a_2+a_3+\cdots$. Let $s_n$ be partial sums for $a$'s and $t_n$ be partial sums for $b$'s. Then both $(s_n)$ and $(t_n)$ are increasing sequences and $\lim s_n=s$. So $s_n\le s$. Also
\[t_n=b_1+b_2+\cdots+b_n=a_{\pi(1)}+a_{\pi(2)}+\cdots+a_{\pi(n)}\]
So we also have $t_n \le s_{\max\{\pi(i)|i=1,\cdots,n\}}\le s$. So we know $t_n$ converges. Say $\lim t_n=t$. Then $t\le s$. But similarly
\[s_n=a_1+a_2+\cdots+a_n=b_{\pi^{-1}(1)}+\cdots+b_{\pi^{-1}(n)} \le t_{\max\{\pi^{-1}(i)|i=1,\cdots,n\}}\]
We can conclude in a similar manner that $s\le t$. So we have $s=t$.

- Generally speaking. We can consider the positive part and negative part of $a_n$ to be $a_n^+:=\frac{a_n+|a_n|}{2}$ and $a_n^-:=\frac{-a_n+|a_n|}2$. This is:

\begin{equation}
a_n^+:=\left\{
\begin{aligned}
&a_n & a_n\ge 0\\
&0   & a_n < 0
\end{aligned} \right.\\

a_n^-:=\left\{
\begin{aligned}
&0 & a_n\ge 0\\
&-a_n & a_n<0
\end{aligned}
\right.
\end{equation}

By comparison test both $\sum a_n^+$ and $\sum a_n^-$ converges (absolutely) because $0\le a_n^+\le|a_n|$ and $0\le a_n^-\le|a_n|$. Let $a^+:=\sum a_n^+$ and $a^-:=\sum a_n^-$. Note that $a_n=a_n^+-a_n^-$. So
\[s=\sum a_n=\sum \left(a_n^+-a_n^-\right)=a^+-a^-\]

Then if $b_1+b_2+\cdots$ is a rearrangement of $a_1+a_2+\cdots$, let us write $b_n^+$ and $b_n^-$ for positive part and negative part of $b_n$. Then by our previous discussion $b_1^++b_2^++\cdots=a_1^++a_2^++\cdots$ and $b_1^-+b_2^-+\cdots=a_1^-+a_2^-+\cdots$ and $b_n=b_n^+-b_n^-$. So 
$$
b_1+b_2+\cdots=\sum b_n=\sum\left(b_n^+-b_n^-\right)\\
=\sum b_n^+-\sum b_n^-=\sum a_n^+-\sum a_n^-\\
=\sum\left(a_n^+-a_n^-\right)=\sum a_n=s
$$
</div>

<div class="Theorem">[Rearrangement Theorem II]
Let $\sum a_n$ be a conditionally convergent series. Then 

- for any $s\in\mathbb R$, there is a rearrangement $\sum b_n$ such that $\sum b_n=s$.

- there is a rearrangement $\sum b_n$ such that the series diverges to $+\infty$

- there is a rearrangement $\sum b_n$ such that the series diverges to $-\infty$.

</div>

Before we prove this fun little theorem, let us see what these two theorems are saying. The first one says that absolute convergent series can only converge to one number. The second theorem says that for a series that converges but not absolutely, by rearranging them the series can converge to anywhere or nowhere. This is the meaning of what we call conditional convergence. The convergence dependes on ordering but absolute convergence is independent of ordering.

<div class="Proof">

- Let us prove the cases for which none of $a_n$ is zero and $s>0$.

- Let $(a_{n_k})$ and $a_{m_j}$ be the subsequences of postive numbers and subsequences of negative numbers respectively.

- Then both subsequences diverges: for otherwise

  * they both converges
  * $(a_{n_k})$ converges but $(a_{m_j})$ diverges
  * $(a_{n_k})$ diverges but $(a_{m_j})$ converges

  but none is possible

  * then $\sum_n |a_n|=\sum_k a_{n_k}-\sum_j a_{m_j}$ converges. But we assumed $\sum_n a_n$ conditionally converges.
  * $\sum_n a_n=\sum_k a_{n_k}-\sum_j a_{m_j}$ diverges
  * $\sum_n a_n=\sum_k a_{n_k}-\sum_j a_{m_j}$ diverges

- Since both subsequeces are monotone then they diverge to $+\infty$ and $-\infty$ respectively.

- Then there exists $k_1$ such that 
$$
a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1-1}}<s\le a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1}}
$$

- Then there exists $j_1$ such that (remember)
$$
a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1}}+\\a_{m_1}+a_{m_2}+\cdots+a_{m_{j_1+1}} \\< s \le\\ a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1}}+\\a_{m_1}+a_{m_2}+\cdots+a_{m_{j_1}}
$$

- Continue this way we get our rearrangement:

$$
\begin{equation}
\pi(i)=\left\{
\begin{aligned}
& n_i & 1\le i \le k_1\\
& m_{i-k_1} & k_1 < i \le k_1+j_1\\
& n_{i-j_1} & k_1+j_1 < i \le k_1+j_1+k_2\\
& \cdots
\end{aligned}
\right.
\end{equation}
$$

- Note that $|s-(a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1}})|<a_{n_{k_1+1}}$ and $|s-(a_{n_1}+a_{n_2}+\cdots+a_{n_{k_1}}+a_{m_1}+a_{m_2}+\cdots+a_{m_{j_1}})|<-a_{m_{j_1+1}}$ etc. And as $\sum_n a_n$ converges we have $\lim_n a_n=0$ so $\lim_k a_{n_k}=\lim_j a_{m_j}=0$. So the sum converges to $s$.

- For $\sum a_n$ with some $a_n=0$. If number of zero terms is finite say there are $m$ zeros, first make $\sum b_n$ be the rearrangement which move all $0$ to the front. Then we can think $\sum_{n=m+1}^\infty b_n$. The use the argument above to rearranage $b_n$ from $m+1$ onwards and keep the front zero the same. If the number of zero terms is infinite. We can use the above argument to rearrange nonzero terms to $b_1+b_2\cdots$ such that $\sum b_n=s$. Then define $c_{2k+1}=b_k$ and $c_{2k}=0$. Then we rearranged all of $a_n$ and $\sum c_n = s$.

- For $s<0$ and diverging to $\pm\infty$, proof is similar.

</div>

If you find the above theorem hard to follow. Don't worry too much about it. I wasn't very careful when proving the theorem. This theorem is more of a demonstration or warning that don't rearrange a conditional convergence sequence.

## Cauchy product of series

<div class="Theorem">
Let $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$ be two convergent series with sum $A$ and $B$ with at least one of them being absolutely convergent. Then the series $\sum c_n$ converge to $AB$ where
$$
c_n=\sum_{i=0}^na_ib_{n-i}
$$
</div>

<div class="Proof">
Assume $\sum a_n$ is the abosulte convergent one.
Let us write $A_n$ for partial sums of $\sum_n a_n$ and $B_n$ for partial sums of $\sum_n b_n$ and $S_n$ be partial sums for $\sum_n c_n$.

- $AB = (A-A_n)B+A_nB$
- $S_n= \sum_{k=0}^n a_kB_{n-k}$ because $\sum_{k=0}^n a_kB_{n-k}=\sum_{k=0}^n a_k\sum_{i=0}^{n-k}b_i=\sum_{k=0}^n\sum_{i=0}^{n-k}a_kb_i=\sum_{k=0}^nc_k$.
- So $AB-S_n=(A-A_n)B+A_nB-\sum_{k=0}^n a_kB_{n-k}=(A-A_n)B+\sum_{k=0}^{n}a_k(B-B_{n-k})$
- We know that $\lim_n(A-A_n)=0$
- If we set $N:=[\frac n2]$. Then $\sum_{k=0}^{n}a_k(B-B_{n-k})=\sum_{k=0}^{N}a_k(B-B_{n-k})+\sum_{k=N+1}^{n}a_k(B-B_{n-k})$.
- $|\sum_{k=0}^{N}a_k(B-B_{n-k})|\le\sum_{k=0}^{N}|a_k||(B-B_{n-k})|\le\max\{B-B_k|N<k\le n\}\sum_{k=0}^N|a_k|$. Since $\lim_k B_k=B$, we have that $(B-B_k)$ is bounded and $\max\{B-B_k|N<k\le n\}$ converging to $0$ (Cauchy criterion of convergent series), then $|\sum_{k=0}^{N}a_k(B-B_{n-k})|$ converges to 0. $|\sum_{k=N+1}^{n}a_k(B-B_{n-k})|\le\sum_{k=N+1}^{n}|a_k||(B-B_{n-k})|\le M\sum_{k=N+1}^\infty|a_k|$. Since $\sum |a_n|$ converges we have $\lim_n|\sum_{k=N+1}^{n}a_k(B-B_{n-k})| = 0$. So $\lim_n|\sum_{k=0}^{n}a_k(B-B_{n-k})|=0$
- So $\lim_n (AB-S_n)=(\lim_n(A-A_n))B+\lim_n \sum_{k=0}^{n}a_k(B-B_{n-k})=0$.
</div>

<div class="Example">
Last time we proved
(In this example we are writing $0^0=0$, so the series below can start with indices $0$ and make everything slightly easier.)
$$
e(x):=\sum_{n=0}^\infty\frac1{n!}x^n
$$
converges absolutely for all $x\in\mathbb R$.

Let us compute $e(x)e(y)=\left(\sum_{n=0}^\infty\frac1{n!}x^n\right)\left(\sum_{n=0}^\infty\frac1{n!}y^n\right)$. Then the $n$-th term of $e(x)e(y)$ is

$$
\sum_{i=0}^n\frac{x^i y^{n-i}}{i!(n-i)!}
$$
The $n$-th term of $e(x+y)$ is

$$
\frac{(x+y)^n}{n!}=\frac1{n!}\sum_{i=0}^n{n\choose i}x^iy^{n-i}=\frac1{n!}\sum_{i=0}^n\frac{n!}{i!(n-i)!}x^iy^{n-i}=\sum_{i=0}^n\frac{x^i y^{n-i}}{i!(n-i)!}
$$
So we have $e(x)e(y)=e(x+y)$.
</div>

## Power series

<div class="Definition">
A power series is a series of the form $\sum_n a_nx^n$ where $a_n$ and $x$ are real numbers
</div>

<div class="Example">
$$
\sum_{n=0}^\infty\frac1{n!}x^n
$$ 
is a power series convergent for all $x\in\mathbb R$.
</div>

<div class="Example">
$$
\sum_{n=1}^\infty n^nx^n
$$
is a power series convergent only for $x=0$. Indeed otherwise $\lim n^nx^n\ne 0$.
</div>

<div class="Example">
$$
\sum_{n=0}^\infty x^n
$$
converges if and only if $x\in(-1,1)$.
</div>

<div class="Example">
$$
\sum_{i=1}^\infty(-1)^{n-1}\frac{x^n}n
$$
converges if and only if $x\in[-1,1]$
</div>

<div class="Theorem">
Let $r\in\mathbb R$ be such that $\sum a_nr^n$ converges, then the power seires $\sum a_n x^n$ converges absolutely for all $|x|<|r|$.
</div>

<div class="Proof">
If $r=0$ there is nothing to prove. So assume $r\ne0$.
Since $\sum a_n r^n$ converges, we have $\lim a_nr^n=0$. So the sequence $(a_nr^n)$ is bounded, say by $K$. Let $x\in\mathbb R$ with $|x|<|r|$. Let $y:=\frac{|x|}{|r|}$. Then $0\le y<1$. So $|a_n x^n|=|a_n||x|^n=|a_n||r^n|y^n\le Ky^n$. So the series $\sum |a_n x^n|$ converges by comparison test.
</div>

<div class="Theorem">
Let $\sum a_nx^n$ be a power series. Then exactly one of the following occurs:

1. The series converges only when $x=0$;
2. The series converges absolutely for all $x\in\mathbb R$;
3. There is some $r>0$ such that the series converges absolutely for all $x\in\mathbb R$ such that $|x|<r$ and divergent for all $x\in\mathbb R$ such that $|x|>r$.

</div>

<div class="Proof">
Let

$$
E:=\{x\in\mathbb R|x\ge 0\land \sum a_nx^n \mathrm{ converges}\}
$$
Obviously $0\in E$.
If $E=\{0\}$, then 1 is true. Otherwise some $0<\alpha\in E$. Then by the previous theorem the power series converges for all $y\in\mathbb R$ such that $0\le|y|\le\alpha$.
If $E$ is not bounded above then any $0<y\in\mathbb R$ is not an upperbound for $E$. So there is $x\in E$ with $x>y$. So the power series also converges for $y$ by previous theorem. Thus $E=[0,\infty)$. So the series converges absolutely for all $x\in\mathbb R$. So 2 is true in this case. Otherwise, $0<r=\sup E$ exists. Then for any $x\in(-r,r)$. We can find some $y\in E$ such that $|x|<y$. So the $\sum a_n y^n$ converges then $\sum a_n x^n$ converges absolutely by previous theorem. If $|x|>r$. Then there is $y\in\mathbb R$ such that $r<y<|x|$ with $y\notin E$. The series is not convergent at $y$. The the series is not convergent at $x$.Otherwise by previous theorem the series is absolute convergent at $y$ which is not true. So 3 is true.

Naturally only one of the 3 cases can be true.
</div>

<div class="Definition">
The radiius of convergence $R$ of the power series $\sum a_n x^n$ is defined as following:

- if the power series converges for all $x\in\mathbb R$, then $R:=\infty$
- if the power series converges only for $0$, then $R:=0$
- if the power series converges absolutely for $|x|<r$ and diverges for $|x|>r$ for some $0<r<\infty$, then $R:=r$
</div>

<div class="Theorem">
\[R=\frac1{\limsup|c_n^{1/n}|}\]
</div>
<div class="Proof">
- Left as an exercise. See @rudin1964principles Theorem 3.39.
- Or use root test.
</div>

<div classs="Example">
\[1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\cdots\] has radius of convergence $R=\infty$.

Set $y=x^2$ then the series is
\[a_0+a_1y+a_2y^2+\cdots\] where $a_n=\frac{(-1)^n}{(2n)!}$.

Using ratio test we have $\lim\frac{|a_{n+1}y^{n+1}|}{|a_n y^n|}=\lim\frac{|y|}{(2n+1)(2n+1)}=0$. So for any $y\ge 0$, the series converges. So forall $x\in\mathbb R$ the series converges.
</div>

<div class="Example">
$$x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots$$ has infinite radius of convergence.
The proof is similar to the previous one.
</div>

<div class="Example">
$$\sum_{n=0}^\infty\frac{(-1)^nx^n}{2n+1}$$ has radius of convergence 1.

We use ratio test:
$$\lim\frac{|a_{n+1}x^{n+1}|}{|a_n x^n|}=\lim\frac{|x|(2n+1)}{2n+3}=|x|$$
So the series is absolutely convergent if $|x|<1$ and diverges if $|x|>1$.

For the sake of completeness let us see what happens when $|x|=1$.
For $x=1$ we have $1-\frac13+\frac15-\frac17+\cdots$which converges by the alternating series test.
For $x=-1$ we have $1+\frac13+\frac15+\frac17\cdots$which diverges by the comparison test with the harmonic series.
</div>

## Summary

- This lecture with last lecture concludes all we want to talk about series;
- We learned test for alternating series;
- We learned that it is safe to rearrange absolutely convergent series;
- We learned that it is dangerous to rearrange conditionally convergent series;
- We learned (basic) power series and radius of convergence.



# Basic Topology on $\mathbb R^k$ (Lecture 8)

Last lecture we conclude our study on sequences and series. From this lecture, we will focus on functions and properties of functions such as continuity, differentiability, integrability etc. In this lecture we will focus on the topology of $\mathbb R^k$ to make a strong foundation for study of functions.

## Metric Spaces

<div class="Definition">
A set $X$ whose elements we shall call points is said to be a metric space if for any points $p$ and $q$ there is a real number $d(p,q)$ called the distance from $p$ to $q$, such that

- [M1] $d(p,q)>0$ if $p\ne q$
- [M2] $d(p,p) = 0$;
- [M3] $d(p,q)\le d(p,r)+d(r,q)$

Any function $X\times X\to \mathbb R$ satisfying above is called a metric on $X$.
</div>

<div class="Example">
The most important examples of metric spaces is $\mathbb R^k$ with the distance between $x=(x_1,\dots,x_k)$ and $y=(y_1,\dots,y_k)$ to be $|x-y|$ where $|x|=\sqrt{\sum_{i=1}^k x_i^2}$

Let us check it is a metric
- M1, M2 are trivial
- M3 is a consequence of the Cauchy-Schwarz inequality:

<div class="Theorem">[Cauchy-Schwarz inequality]
$$
|x\cdot y|\le|x||y|
$$
where $x\cdot y=\sum_{i=1}^k x_iy_i$
</div>

<div class="Proof">
Let us write $A=\sum |x_i^2|$, $B=\sum|y_i^2|$ and $C=\sum x_i y_i$. If $B=0$, then $y_1=y_2=\dots=y_k=0$ and there is nothing to prove.
Otherwise $B > 0$, then
$$
\begin{aligned}
0\le \sum|Bx_i-Cy_i|^2&=\sum(Bx_i-Cy_i)(Bx_i-Cy_i)\\&=\sum B^2|x_i^2|-BC\sum x_i y_i-BC\sum x_i y_i+C^2\sum |y_i|^2\\ &= B^2A-BC^2-BC^2+C^2B\\
&=B(AB-C^2)
\end{aligned}
$$
Thus $0\le B(AB-C^2)$ then $0 \le AB-C^2$ or $C^2\le AB$. So
$$
|\sum x_i y_i|\le \sqrt{\sum |x_i^2|}\sqrt{\sum|y_i^2|}
$$
This is exactly what we want.
</div>
So from the Cauchy-Schwarz inequality we have
$$
\begin{aligned}
|x+y|^2&=(x+y)\cdot(x+y)\\
       &=x\cdot x+2x\cdot y+y\cdot y\\
       &\le x\cdot x+2|x\cdot y|+y\cdot y\\
       &= |x|^2+2|x||y|+|y|^2\\
       &=(|x|+|y|)^2
\end{aligned}
$$
Thus $|x+y|\le |x|+|y|$. Then $|x-y|=|x-z+(z-y)|\le|x-z|+|z-y|$ as required.
</div>

<div class="Example">
If $(X, d)$ is a metric space and $Y\subseteq X$, then $(Y, d|_{Y\times Y})$ is also a metric space.
</div>

## Topology induced by the metric

<div class="Definition">

- If $a_i<b_i$ for $i=1,\dots,k$, then the set of all points $x=(x_1,\dots,x_k)\in\mathbb R^k$ where $x_i\in(a_i,b_i)\forall i$ is called a $k$-cell. (So $1-cell$ is closed interval, $2-cell$ is a rectangle etc).
- If $x\in\mathbb R^k$ and $r>0$, the open (or closed) ball $B$ with center at $x$ and radius $r$ is the set of all $y\in\mathbb R^k$ such that $|x-y|<r$ (or $|x-y|\le r$).
- $E\subset \mathbb R^k$  is convex if $\forall x, y\in E,\forall \lambda\in(0,1)$ we have $\lambda x+(1-\lambda)y\in E$
</div>

<div class="Example">
balls are convex. For if $|y-x|<r$ and $|z-x|<r$ and $0<\lambda<1$ we have
$$
\begin{aligned}
|\lambda y+(1-y)z - x| &= |\lambda (y-x)+(1-\lambda)(z-x)| \\
                       &\le \lambda |y-x|+(1-\lambda)|z-x| \\
                       &\le \lambda r+(1-\lambda) r\\
                       &=r
\end{aligned}
$$
Similarly cells are convex as well.
</div>

<div class="Definition">
Let $(X,d)$ be a metric space. All points and sets mentioned below are understood to be elements and subsets of $X$.

- A neighborhood of $p$ is a set $N_r(p)$ consisting of all $q$ such that $d(p,q)<r$ for some $r>0$. $r$ is called the raius of $N_r(p)$.
- A point $p$ is a limit point of a set $E$ if every neighborhood of $p$ contains a point $q\ne p$ such $q\in E$.
- If $p\in E$ but is not a limit point of $E$, then $p$ is called an isolated point of $E$
- $E$ is closed iff every limit point of $E$ is a point of $E$
- A point $p$ is an interior point of $E$ if there is a neighborhood $N$ of $p$ such that $N\subseteq E$.
- $E$ is open iff every point of $E$ is an interior point of $E$.
- $E$ is perfect iff $E$ is closed and every point of $E$ is a limit point of $E$
- $E$ is bounded if there is real number $M$ and a point $q\in X$ such that $\forall p\in E, d(p,q)<M$.
- $E$ is dense in $X$ if every point of $X$ is either a limit point of $E$ or a point of $E$ (or both).
</div>

<div class="Theorem"> Every neighborhood is an open set
</div>

<div class="Proof">
Consider $E=N_r(p)$ and $q\in E$. Then there is a positive real number $h$ such that $d(p,q)=r-h$. For all points $s$ such that $d(q,s) < h$ we have
$$
d(p,s)\le d(p,q) + d(q,s) < r-h+h=r
$$
</div>

<div class="Theorem">
If $p$ is a limit point of a set $E$, then every neighbourhood of $p$ contains infinitely many points of $E$
</div>
<div class="Proof">
Suppose there is a neighbourhood $N$ of $p$ which contains only a finite number of points of $E$. Let $q_1,\dots,q_n$ be the points of $N\cap E$ that is not $p$, put $r = \min_{1\le m\le n} d(p,q_m)$. Then $r>0$. The neighbourhood $N_r(p)$ contains no point $q$ of $E$ such that $q\ne p$. Then $p$ is not a limit point of $E$.
</div>

<div class="Corollary">
A finite set hence has no limit points. Hence a finite set must be closed.
</div>

<div class="Example">
Let us consider the following subsets of $\mathbb R^2$

1. The set of all $z\in \mathbb R^2$ such that $|z|<1$.
2. The set of all $z\in \mathbb R^2$ such that $|z|\le 1$.
3. A nonempty finite set.
4. $\mathbb Z$
5. $E=\{\frac 1 {n+1}|n\in\mathbb N\}$. Then $0$ is a limit point of $E$ but no points of $E$ is a limit point of $E$.
6. $\mathbb R^2$
7. $\{x|a< x < b\}$

Then

|   |Closed|Open|Perfect|Bounded|
|:-:|:----:|:--:|:-----:|:-----:|
|1  | No   |Yes | No    | Yes   |
|2  | Yes  | No | Yes   | Yes   |
|3  | Yes  | No | No    | Yes   |
|4  | Yes  | No | No    | No    |
|5  | No   | No | No    | Yes   |
|6  | No   | *  | No    | Yes   |

the segment $(a,b)$ is open as a subset of $\mathbb R$ but not open as $\mathbb R^2$
</div>

<div class="Theorem">
A set $E$ is open if and only if its complement is closed
</div>
<div class="Proof">
Suppose $E^c$ is closed. Let $x\in E$, then $x\notin E^c$ and $x$ is not a limit point of $E^c$. Hence there is a neighbourhood $N$ of $x$ such that $E^c\cap N$ is empty, this is $N\subseteq E$. Hence $x$ is an interior point of $E$. So $E$ is open.

Next suppose $E$ is open. Let $x$ be a limit point of $E^c$. Then every neighbourhood of $x$ contains a point of $E^c$. Then $x$ is not an interior point of $E$. Since $E$ is open, then $x\in E^c$. Hence $E^c$ is closed.
</div>

<div class="Corollary">
A set $F$ is closed if and only if its complement is open
</div>

Recall the generalised de Morgan Law:
$$
\left(\bigcup_\alpha E_\alpha\right)^c=\bigcap_\alpha E_\alpha^c
$$

<div class="Theorem">

1. For any collection $\{G_\alpha\}$ of open sets, $\bigcup_\alpha G_\alpha$ is open; 
2. For any collection $\{F_\alpha\}$ of closed sets, $\bigcap_\alpha F_\alpha$ is closed;
3. For any finite collection $G_1,\dots,G_n$ of open sets, $\bigcap_{i=1}^n G_i$ is open;
4. For any finite collection $F_1,\dots,F_n$ of closed sets, $\bigcup_{i=1}^n F_i$ is closed.

</div>

<div class="Proof">

1. For simplicity, write $G=\bigcup_\alpha G_\alpha$. If $x\in G$, then for some $\alpha$ $x\in G_\alpha$. Then $x$ is an interior point of $G_\alpha$ hence an interior point of $G$. Hence $G$ is open.
2. Then $\{F_\alpha^c\}$ is a collection of open sets, then $\bigcup_\alpha F_\alpha^c$ is open then $(\bigcup_\alpha F_\alpha^c)^c$ is closed, but $(\bigcup_\alpha F_\alpha^c)^c=\bigcap_\alpha (F_\alpha^c)^c=\bigcap_\alpha F_\alpha$.
3. For simplicity write $H=\bigcap_{i=1}^n G_i$. For any $x\in H$, there exist neighbourhoods $N_i$ of $x$ with radii $r_i$ such that $N_i\subseteq G_i$. Let $r=\min(r_1,\dots,r_n)$ and let $N$ be the neighbourhood of $x$ with radius $r$. Then $N\subseteq N_i\subseteq G_i$ for all $i=1,\dots,n$. Hence $N\subseteq H$, so $H is open$.
4. Then $F_i^c$ are open, then $\bigcap_{i=1}^nF_i^c$ is open then $\left(\bigcap_{i=1}^nF_i^c\right)^c$ is closed. $\left(\bigcap_{i=1}^nF_i^c\right)^c=\bigcup_{i=1}^n\left(F_i^c\right)^c=\bigcup_{i=1}^nF_i$
</div>

<div class="Example">
In 3 and 4 of previous theorem, the finiteness condition is essential. For example, let $G_n:=\left(-\frac1n,\frac1n\right)$ for $n=1,\dots,$. Then $G_n$ is open as a subset of $\mathbb R$ for all $n$. But $G=\bigcap_{n=1}^\infty G_n$ contains only $0$ and therefore is not open.

Similarly union of an infinite collection of closed sets need not to be closed. For example for each point $x\in\mathbb Q$, $\{x\}$ is closed but $\mathbb Q$ is not. For example $\pi$ is a limit point of $\mathbb R$ but not a rational number. ($\pi$ being irrational is actually hard to prove.)
</div>

<div class="Definition">
Let $(X,d)$ be a metric space and $E\subseteq X$ and let $E'$ be the set of all limit points of $E$ in $X$, then the closure of $E$ is $\bar E:=E\cup E'$.
</div>

<div class="Theorem">
Let $(X,d)$ be a metric space and $E\subseteq X$, then

1. $\bar E$ is closed;
2. $E=\bar E$ if and only if $E$ is closed;
3. $\bar E\subseteq F$ for every closed set $F\subseteq X$ such that $E\subseteq F$.
</div>

<div class="Proof">

1. Let $p\in X$ such that $p\notin \bar E$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has neighbourhood which does not intersect $E$. The complement of $\bar E$ is therefore open. Hence $\bar E$ is closed.
2. left to right trivial by 1; If $E$ is closed then $E'\subseteq E$ then $\bar E=E$.
3. If $F$ is closed and $E\subseteq F$, then $F'\subseteq F$ then $E'\subseteq F$ then $\bar E\subseteq F$.
</div>

By previous theorem $$\bar E=\bigcap_{F\in\mathcal P(X)\\ F \textrm{ closed}\\ E\subseteq F} F$$

<div class="Theorem">
Let $E$ be a nonempty set of real numbers which is bounded above. Let $y=\sup E$. Then $y\in\bar E$. Hence $y\in E$ if $E$ is closed
</div>

<div class="Proof">
If $y\in E$ then certainly $y\in\bar E$. Assume $y\notin E$. For every $h>0$, there exists a point $x\in E$ such that $y-h < x < y$, for otherwise $y-h$ would be an upper bound of $E$. Thus $y$ is a limit point of $E$. Then $y\in\bar E$.
</div>

<div class="Remark">
Let $(X, d)$ be a metric space, suppose that $E\subseteq Y\subseteq X$. Then $Y$ is also a metric space by restricting $d$ to $Y\times Y$. Then to say $E$ is open in $X$ means that to each point $p\in E$ there is associated a positive number $r$ such that $\forall q\in X,d(p,q)< r \implies q\in E$. We can also say $E$ is an open subset of $Y$ or open relative to $Y$ that is for each $p\in E$ there is associated a positive number $r$ such that $\forall q\in Y, d(p,q)< r\implies q\in E$. We saw in a previous example that, open relative to $Y$ doesn't necessarily mean open relative to $X$. But we still have the following theorem
</div>

<div class="Theorem">
Suppose $Y\subseteq X$. $E\subseteq Y$ is open relative to $Y$ if and only if $E=Y\cap G$ for some $G\subseteq X$ open in $X$.
</div>
<div class="Proof">
From right to left is easier: if $G$ is open in $X$ and $E=G\cap Y$ then, for every $p\in E$ has a neighbourhood $V_p\subseteq G$. Then $V_p\cap Y\subseteq E$ and $V_p\cap Y\subseteq E$. So $E$ is open relative to $Y$.
</div>

## Compact sets

<div class="Definition">

- By an open cover of a set $E$ in a metric space $(X, d)$ we mean a collection $\{G_\alpha\}$ of open subsets of $X$ such that $E\subseteq \bigcup_\alpha G_\alpha$.
- A subset $K\subseteq X$ is said to be compact if and only if every open cover of $K$ contains a finite subcover. More explicitly, if $\{G_\alpha\}$ is an open cover of $K$, then there are finitely many indices $\alpha_1,\dots,\alpha_n$ such that $K\subseteq G_{\alpha_1}\cup\dots\cup G_{\alpha_n}$.

</div>

Obviously every finite subset is compact.

We have seen that $E\subseteq Y\subseteq X$ then $E$ might be open relative $Y$ without being open relative to $X$. However compactness behaves better:

<div class="Theorem">
Suppose $K\subseteq Y\subseteq X$. Then $K$ is compact relatvie to $X$ if and only if $K$ is compact relative to $Y$.
</div>

Because of this theorem, we are able to in many situations to regard compact sets as metric spaces in their own right without paying attention to any embedding space. In particular althought it makes little sense to talk about open spaces or closed spaces (every metric space $X$ is an open and closed subset of itself), however it does make sense to talkt about compact metric spaces.

<div class="Proof">
Suppose $K$ is compact relative to $X$ and let $\{V_\alpha\}$ be a collection of sets open relative to $Y$ such that $K\subseteq \bigcup_\alpha V_\alpha$. Then there are sets $G_\alpha$ open relative to $X$ such that $V_\alpha=Y\cap G_\alpha$ for all $\alpha$. Since $K$ is compact relative to $X$, we can find some finitely many indices $\alpha_1,\dots,\alpha_n$ such that $K\subseteq G_{\alpha_1}\cup\dots\cup G_{\alpha_n}$. Then $K\subseteq V_{\alpha_1}\cup\dots\cup V_{\alpha_n}$. We have found a finite subcover of $\{V_\alpha\}$.

Conversely, suppose $K$ is compact relative to $Y$ and $\{G_\alpha\}$ is a collection of open subsets of $X$ which covers $K$. Put $V_\alpha=Y\cap G_\alpha$. Then there are some finitely many indeces $\alpha_1,\dots,\alpha_n$ such that $K\subseteq V_{\alpha_1}\cup\dots\cup V_{\alpha_n}$. Then certainly $K\subseteq G_{\alpha_1}\cup\dots\cup G_{\alpha_n}$ because $V_{\alpha_i}\subseteq G_{\alpha_i}$.
</div>

<div class="Theorem">
Compact subsets are closed.
</div>
<div class="Proof">
Let $K$ be compact. We will prove $K^c$ open. Suppose $p\notin K$. If $q\in K$. let $V_q$ and $W_q$ be neighbourhoods of $p$ and $q$ with radius less than $\frac12 d(p,q)$. Then $K$ is covered by $\{W_q|q\in K\}$. Hence we can find finitely many $q_1,\dots,q_n\in K$ sucht that $K\subseteq W_{q_1}\cup\dots\cup W_{q_n}=: W$. Write $V:= V_{q_1}\cap\dots\cap V_{q_n}$, then $V$ is a neighbourhood of $p$ which does not intersect $W$. Hence $V\subseteq K^c$. Then $p$ is an interior point.
</div>

<div class="Theorem">
Closed subsets of compact subset is compact.
</div>
<div class="Proof">
Suppose $F\subseteq K\subseteq X$, $F$ is closed relative to $X$ and $K$ is compact. Let $\{V_\alpha\}$ be an open cover of $F$. If $F^c$ is adjoined to $\{V_\alpha\}$ we have another open cover $\Omega $ of $K$. Since $K$ is compact there is a finite subcolelction $\Phi$ of $\Omega$ covering $K$, hence covering $F$. If $F^c$ is a memember of $\Phi$, then $\Phi-{F^c}$ still covers F. We have thus found a finite subcover.
</div>

<div class="Corollary">
If $F$ is closed and $K$ compact, then $F\cap K$ is compact.
</div>

<div class="Theorem">
If $\{K_\alpha\}$ is a collection of compact subsets of a metric space $X$ such that every finite subcollection of $\{K_\alpha\}$ is nonempty then $\bigcap K_\alpha$ is nonempty.
</div>

<div class="Proof">
Fix a member $K_1$ of $\{K_\alpha\}$ and put $G_\alpha=K_\alpha^c$. For contradiction assume that no point of $K_1$ belongs to every $K_\alpha$. Then the sets $G_\alpha$ is an open cover of $K_1$, there are finitely many indices $\alpha_1,\dots,\alpha_n$ such that $K_1\subseteq G_{\alpha_1}\cup\dots\cup G_{\alpha_n}$. Then $K_1\cap K_{\alpha_1}\cap\dots\cap K_{\alpha_n}$ is empty.
</div>

<div class="Corollary">
If $\{K_n\}$ is a sequence of nonempty compact sets such that $K_{n+1}\subseteq K_n$, then $\bigcap_{i=1}^\infty K_n$.
</div>

<div class="Theorem">
If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$.
</div>

<div class="Proof">
Otherwise, each $q\in K$ would have a neighbourhood $V_q$ which only intersect $E$ at $q$. Clearly no finite subcollection of $\{V_q\}$ can cover $E$. Same is true for $K$ since $E\subset K$. But $K$ is compact.
</div>

 Let us recall one of the previous theorems, namely nested interval principle:

 <div class="Theorem">
 If $\{I_n\}$ is a sequence in $\mathbb R^1$ such that $I_{n+1}\subseteq I_n$ then $\bigcap I_n$ is not empty.
 </div>
 <div class="Proof">
 Write $I_n=[a_n,b_n]$. Let $E=\{a_n\}$. Then $E$ is nonempty and bounded above by $b_1$. Let $x=\sup E$. If $m,n$ are positive integers, then 
 $$
  a_n\le a_{m+n}\le b_{m+n} \le b_m,
 $$
 so that $x\le b_m$ for each $m$. Since it is obvious that $a_m\le x$, we see that $x\in I_m$ for $m=1,2,3\dots$.
 </div>

 <div class="Theorem">
 Let $k$ be a positive integer. If $\{I_n\}$ is a sequence of $k-cells$ such that $I_{n+1}\subseteq I_n$. Then $\bigcap_1^\infty I_n$ is nonempty
 </div>
<div class="Proof">
The proof has the idea as the above, mutatis mutandis.
Let $I_n$ be the cells of points $x=(x_1,\dots,x_k)$ such that for all $1\le j\le k; n=1,2,3,\dots$
$$
  a_{n,j}\le x_j\le b_{n,j}
$$ 

Write $I_{n,j}=[ a_{n,j}, b_{n,j}]$. For each $j$, by previous theorem there is a real number $x_j^*\in\bigcap_n I_{n,j}$. Then $(x_1^*,\dots,x_k^*)\in I_n$ for all $n$.
</div>

All of above is to prove
<div class="Theorem">
Every $k$-cell is compact
</div>
<div class="Proof">
Let $I$ be a $k$-cell, consisting of all points $x=(x_1,\dots,x_k)$ such that $a_j\le x_j\le b_j$ for all $1\le j\le k$. Put
$$
  \delta=\sqrt{\sum_1^k (b_j-a_j)^2}.
$$
Then for all $x,y\in I, |x-y|\le \delta$.

Suppose for contradiction that there exists an open cover $\{G_\alpha\}$ of $I$ without subcover of $I$. Put $c_i=\frac{a_j+b_j}2$. Then the intervals $[a_j, c_j]$ and $[c_j, b_j]$ determines $2^k$ $k$-cells $Q_i$ whose union is $I$. At least one of $Q_i$ call it $I_1$ cannot be covered by finite subcollection of $\{G_\alpha\}$. Otherwise  $I$ can be covered by a finite cover. We can continue to divide $I_1$ and continue the whole process. We then obtain a sequnce of $\{I_n\}$ such that

1. $\dots\subseteq I_3\subseteq I_2\subseteq I_1\subseteq I$;
2. $I_n$ is not covered by any finite subcollection of $\{G_\alpha\}$;
3. if $x, y\in I_n$, then $|x-y|\le 2^{-n}\delta$.

Then by previous theorem, there is a point $\iota$ in every $I_n$. For some $\alpha$, $\iota\in G_\alpha$. Since $G_\alpha$ is open, there is some $r>0$ such that $|y-\iota|< r$ implies that $y\in G_\alpha$. Then for $n$ large enough $2^{-n}\delta< r$ then 3 would implies $I_n\subseteq G_\alpha$ contradicting 2.
</div>

<div class="Theorem">[Heine-Borel]
If a set $E\subseteq \mathbb R^k$, TFAE:

1. $E$ is closed and bounded;
2. $E$ is compact;
3. Every infinite subset of $E$ has a limit point in $E$.
</div>
<div class="Proof">

- Suppose 1, then $E\subseteq I$ for some $k$-cell $I$. Then $E$ is compact as a closed subset of compact set $I$.
- 2 implies 3 is a previous theorem
- Let us now prove 3 implying 1.
  * Suppose $E$ is not bounded, then $E$ contains distinct points $x_n$ such that $|x_n|>n$. Then $\{x_n\}$ is a set without limit point. Then 3 implies $E$ being bounded.
  * If $E$ is not closed, then there is a point $x_0\in\mathbb R^k$ which is a limit point of $E$ but not a point of $E$. For $n=1,2,3,\dots$, there are points $x_n\in E$ such that $|x_n-x_0|<\frac1n$. Let $S=\{x_n\}$. Then $S$ is infinite (for otherwise $|x_n-x_0|$ would have a constant postitive value, for infinitely many $n$). $S$ has $x_0$ as limit point and no other limit point in $\mathbb R^k$. For if $y\ne x_0$ then
  $$
  \begin{aligned}
  |x_n-y|&\ge|x_0-y|-|x_n-x_0|\\
         &\ge|x_0-y|-\frac1n\ge\frac12|x_0-y|
  \end{aligned}
  $$
  for all but finitely many $n$; this shows that $y$ is not alimit point of $S$ (otherwise there should be infintely many points in intersection by a previous theorem).
Thus $S$ has no limit point in $E$; then $E$ must be closed if 3 holds.
</div>

We can now generalise a theorem proved before
<div class="Theorem">[Weierstrass] Every bounded infinite subset of $\mathbb R^k$ has a limit in $\mathbb R^k$.
</div>
<div class="Proof">
Being bounded, the set $E$ is a subset of $k$-cell $I\subseteq \mathbb R^k$. $I$ is compact, then $E$ has a limit point in $I$.
</div>

## Perfect set

<div class="Theorem">
Every nonempty perfect set in $\mathbb R^k$ is uncountable.
</div>
<div class="Proof">
Since $P$ has limit points, $P$ must be infinite. If $P$ is countable, we can enumerate $P$ by $x_1,x_2,x_3\dots$. Let us construct a sequence $\{V_n\}$ of neighbourhoods as following:

- Let $V_1$ be arbitrary neighbourhood of $x_1$. If $V_1$ consists of all $y\in\mathbb R^k$ such that $|y-x_1|< r$, the closure $\bar {V_1}$ is the set of all points $y\in\mathbb R^k$ such that $|y-x_1|\le r$.
- Suppose $V_n$ has been constructed, so that $V_n \cap P$ is not empty. Since every point of $P$ is a limit point of $P$. There is a neighbourhood $V_{n+1}$ such that
  1. $\bar{V}_{n+1}\subseteq V_n$
  2. $x_n\notin \bar{V}_{n+1}$
  3. $V_{n+1}\cap P$ is nonempty.
Hence by induction we can construct a sequence of $\{V_n\}$ such that each $V_n\cap P$ is nonempty.

Put $K_n=\bar{V}_n\cap P$. Since $\bar{V}_n$ is closed and bounded hence compact. Then $x_n\notin K_{n+1}$. No point of $P$ is in $\bigcap K_n$. But $K_n\subseteq P$. Then $\bigcap K_n$ is empty. Since each $K_n$ is nonempty and $K_{n+1}\subseteq K_n$, this contradicts a previous corollary.
</div>

<div class="Corollary">
Every interval is uncountable. $\mathbb R$ is uncountable.
</div>

## Summary

In this lecture, we are trying to learn basic concepts of topology:

- we defined what is a metric space and what is a metric;
- we defined the topology induced by metrics;
- we discussed compact set and perfect set;
- we generalised some theorems from $\mathbb R^1$ to higher dimensions $\mathbb R^k$;
- We actually proved the uncountability of $\mathbb R$ rigorously.

# Function and continuity (Lecture 9)

Last lecture, we talked about standard topology or Euclidean topology on $\mathbb R^k$. In this lecture, we are going to use this topology to build notions of limit of a function and continuity of function.

## Limit of a function

<div class="Definition">
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces; suppose $E\subseteq X$, $f : E\to Y$ and $p$ is a limit point of $E$. We write $f(x)\to q$ as $x\to p$ or $$\lim_{x\to p} f(x)$$ if there is a point $q\in Y$ such that
$$\forall \epsilon>0\exists\delta>0 
, 0< d_X(x,p)\implies d_Y(f(x),q)<\epsilon$$
</div>

Note that $p$ does not have to be a point of $E$.

<div class="Theorem">[Heine's Definition]
Let $X,Y,f,p,q$ be as above. Then
$$\lim_{x\to p}f(x)=q$$ iff and only if $$\lim_{n\to\infty} f(p_n)=q$$ for every sequence $(p_n)$ in $E$ such that $p_n\ne p$ and $\lim_{n\to\infty} p_n=p$.
</div>
<div class="Proof">

- from left to right: Choose any sequence $(p_n)$ with required property. Let $\epsilon>0 be given$, there is a $\delta>0$ such that for all $x\in E$ and $0 < d_(x,p) < \delta$ we would have $d_Y(f(x),q)<\epsilon$. There is also an $N\in\mathbb N$ such that $\forall n>N$ we have $0< d_X(p_n,p)<\delta$. Thus for all $n>N$ we have $d_Y(f(p_n),q)<\epsilon$. This is what we want.

- from right to left: We prove the contrapositive: assume $\lim_{x\to p}f(x)\ne q$. Then there is some $\epsilon>0$ such that for every $\delta > 0$ there is some $x\in E$ such that $0< d_X(x,p) < \delta$ but $d_Y(f(x),q)\ge \epsilon$. Let $\delta_n=\frac1n$. Then we find a sequence in $E$ satisfies required properties but the limit is wrong.
</div>

<div class="Corollary">
From above theorem and uniqueness of limit of a sequence, limit of a function is unique if exists.
</div>

<div class="Corollary">
From the above theorem and properties of limit, immediately we have:
Let $f,g:E\to\mathbb R$
Suppose $$\lim_{x\to p} f(x)=A,\lim_{x\to p} g(x)=B$$. Then:

1. $\lim_{x\to p} (f+g)(x) = A + B$
2. $\lim_{x\to p} (fg)(x) = AB$
3. $\lim_{x\to p} \left(\frac f g\right)(x)=\frac A B$. Of course provided $g$ and $B$ are non-zero
</div>

<div class="Remark">
If $f,g:E\to\mathbb R^k$ then 1 is true 2 becomes $\lim (f\dot g)=A\dot B$
</div>

## Continuous functions

<div class="Definition">

- Suppose $X$ and $Y$ are metric spaces, $E\subseteq X$ and $p\in E$ and $f$ maps $E$ into $Y$. Then $f$ is said to be continuous at $p$ if for every $\epsilon>0$ there is a $\delta$ such that for all points $x\in E$ with $d_X(x,p)<\delta$, we have $d_Y(f(x),f(p))<\epsilon$.
- If $f$ is continuous at every point of $E$, we say $f$ is continuous on $E$.
</div>

Note that:

- $f$ does not have to be defined at point $p$ for this definition to make sense.
- If $p$ is an isolated point of $E$, then our definition implies that every function which has $E$ as domain is automatically continuous at $p$. Because no matter which $\epsilon>0$, we can always find a $\delta>0$ such that the only point in $E$ and the neighbourhood with radius $\delta$ is $p$. Then $d_Y(f(x),f(p))=0<\epsilon$.

<div class="Theroem">
If $p$ is a limit point of $E$. Then $f$ is continuous at $p$ if and only if $\lim_{x\to p} f(x)=f(p)$
</div>
<div class="Proof">
This is really just rephrasing definition.
</div>

<div class="Theorem">
Suppose $X,Y,Z$ are metric spaces with $E\subset X$, $f:E\to Y$, $g:f(E)\to Z$. Then if $f$ is continuous at a point $p\in E$ and if $g$ is continuous at the point $f(p)$, then $g\circ f$ is continuous at p.
</div>
<div class="Proof">
Let $\epsilon > 0$. Since $g$ is continuous at $f(p)$, there is some $\eta>0$ such that
$$d_Z(g(y), g(f(p))<\epsilon \textrm{if } d_Y(y,f(p))<\eta\textrm{ and }t\in f(E).$$

Since $f$ is continuous at $p$, there is a $\delta>0$ such that
$$d_Y(f(x),f(p))<\eta\textrm{ if }d_X(x,p)<\delta \textrm{ and }x\in E$$.

Combine these, if $d_X(x,p)<\delta$ and $x\in E$. Then
$$d_Z(g\circ f(x), g\circ f(p))<\epsilon.$$ This is continuity at $p$.
</div>

<div class="Theorem">
A mapping $f:X\to Y$ is continuous if and only if $f^{-1}(V)$ is open for every open set $V$ in $Y$.
</div>
<div class="Proof">
- From left to right: suppose $f$ is continuous on $X$ and $V$ is an open set in $Y$. We have to show that $f^{-1}(V)$ is an interior point of $f^{-1}(V)$. Suppose $f(p)\in V$. Then since $V$ open, there is $\epsilon > 0$ such that $y\in V$ if $d_Y(f(p),y)<\epsilon$. Since $f$ is continuous then there is $\delta>0$ such that $d_Y(f(x), f(p))<\epsilon$ if $d_X(x,p)<\delta$. Thus $x\in f^{-1}(V)$ as long as $d_X(x,p)<\delta$.
- From right to left: Fix $p\in X$ and $\epsilon > 0$. Let $V$ be ball of radius $\epsilon$ around $f(p)$. Then $V$ is open then $f^{-1}(V)$ is open. Then there is $\delta > 0$ such that $x\in f^{-1}(V)$ as long as $d_X(p,x)<\delta$. But if $x\in f^{-1}(V)$ then $f(x)\in V$. So $d_Y(f(x),f(p))<\epsilon$
</div>

<div class="Corollary">
A mapping $f:X\to Y$ is continuous if and only if $f^{-1}(C)$ is closed for all closed set $C$ in $Y$.
</div>
<div class="Proof">
A set is closed if and only if its complement is open. But $f^{-1}(E^c)=[f^{-1}(E)]^c$
</div>

<div class="Theorem">
Let $f$ and $g$ be continuous functions from $X \to \mathbb R$. Then $f+g$, $fg$ $f/g$ are continuous.
</div>
<div class="Proof">
At isolated points, nothing to prove. Otherwise use previous theorems, easy.
</div>

<div class="Theorem">

1. Let $f_1,\dots,f_k$ be a real functions on a metric space $X$, let $f:X\to \mathbb R^k$ be defined as
$$f(x)=(f_1(x),\dots,f_k(x)).$$
Then $f$ is continuous if and only if $f_i$ are continuous for all $i=1,\dots,k$.
2. Thus if $f,g:X\to \mathbb R^k$ then $f+g:X\to\mathbb R^k$ and $f\dot g:X\to\mathbb R$ are all continuous.
</div>
<div class="Proof">
From 1 to 2 is trivial. Just prove 1.
Part 1 follows from this inequaly:
$$|f_j(x)-f_j(y)|\le |f(x)-f(y)|=\sqrt{\sum_{i=1}^k |f_i(x)-f_i(y)|^2}$$
for $j=1,\dots,k$.
Thus assuming continuity of $f$ then continuity of $f_i$ is trivial. Conversely for any $\epsilon>0$, there is $\delta_i$ such that $d(x,y)\le \delta_i$ we have $|f_i(x)-f_i(y)|\le \frac{\epsilon^2}k$ hence $|f(x)-f(y)| \le \epsilon$.
</div>

<div class="Example">
If we define $\phi_i (x): \mathbb R^k \to \mathbb R=x_i$. Then $\phi_i$ are continuous. Apply above theorem repeatedly we can show that evaluatiing every multivariable polynomial is continuous:
$$P(x)=\sum c_{n_1\dots n_k}x_1^{n_1}\dots x_k^{n_k}.$$
</div>

<div class="Example">
Since $\left||x|-|y|\right|\le|x-y|$, we have $x\mapsto |x|$ is continuous. Hence if $f:X\to\mathbb R^k$ is continuous, $\phi(x):=|f(x)|$ is continuous.
</div>

<div class="Remark">
Being continuous is a "local" property. In the definition the complement of $E$ never played any rule. Hence nothing of interest is lost if we just think $E$ as a metric space on its own right other than a subset of an ambient space. Similarly we think the image as a metric space on its own (with subspace topology of course). In this way we don't have to pass subsets around.
</div>

## Continuity and compactness

<div class="Definition">
A mapping $f:E\to \mathbb R^k$ is bounded if and only if there is a real number $M$ such that $|f(x)|\le M$ forall $x\in E$.
</div>

<div class="Theorem">
Suppose $f$ is continuous mapping of compact metric space X to compact metric space Y. Then image of $f$ is compact.
</div>

<div class="Proof">
Let $\{V_\alpha\}$ convers $f(X)$. Since $f$ is continuous, $f^{-1}(V_\alpha)$ is open and covers $X$. Hence there is a choice of finitely many indeces $\alpha_1,\dots,\alpha_n$ such that $X\subseteq f^{-1}(V_{\alpha_1})\cup\dots \cup f^{-1}(V_{\alpha_n})$. Then since $f(f^{-1}(E))\subseteq E$ So $f(X)\subseteq V_{\alpha_1}\cup\dots V_{\alpha_n}$. This is everything.
</div>

<div class="Theorem">
If $f$ is continuous mapping from compact $X$ to $\mathbb R^k$ then image is closed and bounded. Hence $f$ is bounded.
</div>
<div class="Proof">
Preivous theorem + Heine-Borel.
</div>

The following is  <span style="font-size:20px;">super important!</span> If you decided just to remember one thing from this lecture, remeber this!

<div class="Theorem">[IMPORTANT]
Suppose $f$ is continuous $X\to \mathbb R$ and $M=\sup f(X)$ and $m=\in f(X)$. Then there exists points $p,q\in X$ such that $f(p)=M$ and $f(q)=m$.
</div>
<div class="Proof">
previous theorem + a theorem from last lecture.
Recall:
<div class="Theorem">
Let $E$ be nonempty set of real numbers which is bounded above. Let $y=\sup E$. Then $y\in\bar E$. Hence $y\in E$ if $E$ is closed.
</div>
Use $E$ to be $f(X)$ which is closed and bounded. $\inf$ is similar.
</div>

<div class="Theorem">Suppose $f$ is a continuous bijection from compact $X$ to (not necessarily compact) $Y$. Then the inverse $f^{-1}:Y\to X$ is continuous
</div>
<div class="Proof">
Suffices to prove that $f(V)$ is open in $Y$ for every open set $V$ in $X$.
Fix $V$. Then $V^c$ is closed hence compact. $f(V^c)$ is compact in $Y$ hence closed in $Y$. Since $f$ bijection $f(V^c)=f(V)^c$ is closed. Then $f(V)$ is open.
</div>

<div class="Example">
Here compactness of $X$ is really important.
Consider $X=[0,2\pi)$ and $Y$ being the unit circle on $\mathbb R^2$ and $f(t)=(\cos t, \sin t)$. Though havn't formally defined trigonometry. Let's just work from intuition. $f$ is indeed bijection. But $X$ is not compact in $\mathbb R$. The inverse function is not continuous because $f^{-1}(1,0)=0$ but just above $(1,0)$ the preimage is close $0$ just below $(1,0)$ preimage is close to $2\pi$. Note $Y$ is compact and this still fails. $X$ being compact is the key.

</div>

## Uniform continuity

This part is potentially confusing, I am not sure how much I would use in future lectures. If you find yourself wandering too much about the difference between uniform continuity and continuity, perhaps save this bit for later. But uniform continuity is nevertheless an important topic and should be understood fully.

<div class="Definition">
Let $f:X\to Y$ be a mapping between metric spaces. We say $f$ is uniformly continuous on $X$ if for every $\epsilon > 0$ there exists $\delta>0$ such that $d_Y(f(p),f(q))<\epsilon$ for all $p, q\in \mathbb X$ for which $d_X(p,q)<\epsilon$.
</div>

For continuity the choice of $\delta$ can be dependent on choice of $p$. But for uniform continuity $\delta$ can only dependent on $\epsilon$ and nothing else.
Trivially, uniform continuity $\implies$ continuity but generally not vice versa unless $X$ is compact. See below.

<div class="Theorem">
Let $f$ be a continuous mapping from compact $X$ to $Y$. Then $f$ is uniformly continuous.
</div>

<div class="Proof">
Let $\epsilon > 0$. Since $f$ is continuous, we can associate each $p\in X$ a positive number $\phi(p)$ such that
$$q\in X, d_X(p,q)<\phi(p) \implies d_Y(f(p), f(q))<\frac\epsilon 2.$$
Let $J(p)$ be the open neighbourhood around $p$ with radius $\frac12 \phi(p)$.
Then $p\in J(p)$ hence the collection of all $\{J(p)\}$ forms an open cover of $X$. Hence admit a finite subcover $X\subseteq J(p_1)\cup\dots\cup J(p_n)$.
Let 
$$\delta=\frac12\min(\phi(p_1),\dots,\phi(p_n)).$$
Then by finiteness $\delta > 0$. Suppose $p,q$ are points of distance less than $\delta$. Then there is an interger $m$ between $1$ and $n$ such that $p\in J(p_m)$. Hence
$$d_X(p, p_m)\le \frac12\phi(p_m)$$
Also,
$$d_X(q, p_m)\le d_X(p,q)+d_X(p,p_m)<\delta+\frac12\phi(p_m)\le\phi(p_m).$$
Then by definition of $\phi$ we have
$$d_Y(f(p),f(q))\le d_Y(f(p),f(p_m))+d_Y(f(q),f(p_m))<\frac\epsilon2+\frac\epsilon2=\epsilon$$
</div>

However,
<div class="Theorem">
If $E$ is noncompact set in $\mathbb R$. Then

1. there is a continuous function on $E$ that is not bounded;
2. there is a continuous and bounded function on $E$ without maximum;
3. If $E$ is bounded, there is a continuous function on $E$ that is not uniformly continuous.
</div>

<div class="Proof">

If $E$ is bounded then $E$ is not closed. Hence there is a limit point $x_0$ of $E$ but not in $E$. Consider
  $$f(x)=\frac1{x-x_0}$$
This is continuous but not bounded. It is not uniformly continuous. Let $\delta>0$ be arbitrary. Choose a point $x\in E$ so that $|x-x_0|<\delta$. Let $t$ be a number close to $x_0$ we can make the difference $|f(t)-f(x)|>1$ while $|t-x|<\delta$. The detail is left as an exercise of algebraic manipulation.

Consider $$g(x)=\frac1{1+(x-x_0)^2}$$ This function is bounded since $0< g(x) < 1$. supremum is $1$ but the supremum is never achieved.

If $E$ is unbounded, then $f(x)=x$ is a continuous function that is not bounded and 
$$h(x)=\frac{x^2}{1+x^2}$$ is a bounded function but supremum is $1$ and this is never attained. This finishes the proof.
</div>

## Discontinuity

If $x$ is a point in the domain of the definition of the function $f$ at which $f$ is not continuous, we say that $f$ is discontinuous at $x$, or that $f$ has a discontinuity at $x$. In this part we are going to classify two types of discontinuities.

<div class="Definition">[One side limit]
Let $f$ be defined on $(a,b)$. Consider any point $x$ such that $a\le x< b$. We write
$$f(x+)=q$$ if $f(t_n)\to q$ as $n\to\infty$ for all sequences $(t_n)$ in $(x,b)$ such that $t_n\to x$. For the definition of $f(x-)$ for $a< x\le b$, we restrict ourselves to sequences $(t_n)$ in $(a,x)$.
</div>

It is clear that any point $x$ of $(a,b)$, then $\lim_{t\to x} f(t)$ exists if and only if
$$f(x+)=f(x-)=\lim_{t\to x} f(t)$$

<div class="Definition">
Let $f$ be defined on $(a,b)$. If $f$ is discontinuous at a point $x$ and if $f(x+)$ an $f(x-)$ exist, then $f$ is said to have a discontinuity of the first kind, or a simple discontinuity at $x$. Otherwise the discontinuity is said to be of the second kind.
</div>

There are two ways in which a function can have a simple discontinuity:

- $f(x+)\ne f(x-)$ in this case the value of $f(x)$ doesn't really matter.
- $f(x+)=f(x-)\ne f(x)$

<div class="Example">

1. Define:
$$
f(x)=\left\{
\begin{aligned}
& 1 & (x\textrm{ rational})\\
& 0 & (x\textrm{ irrational})
\end{aligned}\right.
$$
Then $f$ has a discontinuity of the second kind at every point $x$, since neither $f(x+)$ nor $f(x-)$ exists

2. Define:
$$
f(x)=\left\{
\begin{aligned}
& x & (x\textrm{ rational}) \\
& 0 & (x\textrm{ irrational})
\end{aligned}
\right.
$$
Then $f$ is continuous at $x=0$ and discontinuous of the second kind at every other point.

3. Define:
$$
f(x)=\left\{
\begin{aligned}
& x+2 & (-3< x <-2)\\
& -x-2 & (-2\le x<0)\\
& x+2 & (0\le x<1)
\end{aligned}
\right.
$$
Then $f$ has a simple discontinuity at $x=0$ and is continuous at every other point of $(-3,1)$.

4. Define
$$
f(x)=\left\{
\begin{aligned}
& \sin\frac1x & (x\ne 0)\\
& 0 & (x = 0)
\end{aligned}
\right.
$$
Neither $f(0+)$ nor $f(0-)$ exists, $f$ has a discontinuity of second at $x=0$. Assume the continuity of $\sin$. Then $f(x)$ is continuous at every other points $x\ne 0$. One side limit doesn't exists because $sin(2n\pi)=0$ but $sin\left(\left(2n+\frac12\right)\pi\right)=1$ for all $n\in\mathbb Z$ or something like that. Then by picking $(t_n)=\left(\frac1{2n\pi}\right)$ and $(s_n)=\left(\frac1{\left(2n+\frac12\right)\pi}\right)$. They both tends to $0$ but result in different limits after applying $f$. Similarly for $f(0-)$.
</div>

## Monotonic functions

<div class="Definition">
Let $f:(a,b)\to\mathbb R$. We say $f$ to be monotonically increasing (decreasing resp.) on $(a,b)$ if $a< x < y< b$ implies $f(x)\le f(y)$ ($f(y) \le f(x)$ resp.).
</div>

<div class="Theorem">
Let $f$ be monotonically increasing on $(a,b)$. Then $f(x+)$ and $f(x-)$ exists at every point of $x\in(a,b)$. More precisely,
$$\sup_{a < t < x}f(t)=f(x-)\le f(x)\le f(x+)=\inf_{x < t < b} f(t)$$
Futhermore, if $a < x < y < b$, then
$$f(x+)\le f(y-)$$.

For monotonically decreasing functions, analogous results hold.
</div>
<div class="Proof">
By hypothesis, the set of numbers $f(t)$ where $a < t < x$ is bounded above by $f(x)$. Hence a least upper bound exists, we call is $A$. Then $A\le f(x)$. We need to show $A=f(x-)$.

Let $\epsilon>0$. Then by defniition of $\sup$, there is a $\delta>0$ such that $a< x-\delta < x$ and $$A-\epsilon< f(x-\delta)\le A$$ By monotonicity, for all $x-\delta< t < x$ $$f(x-\delta)\le f(t)\le A$$ Thus we have $$|f(t)-A|\le \epsilon$$ for all $x-\delta < t < x$. Hence we have $f(x-)=A$.
The other half use the same method mutatis mutandis.

Assume $a < x < y < b$, then $$f(x+)=\inf_{x< t < b}f(t)=\inf_{x < t < y}f(t)$$ Similarly we also have $$f(y-)=\sup_{a < t < y} f(t)=\sup_{x< t < y} f(t)$$. This finishes the proof.
</div>

<div class="Corollary">
Monotonic functions have no discontinuity of the second kind.
</div>

<div class="Theorem">
Let $f$ be monotonic on $(a,b)$. Then the set of points of $(a,b)$ at which $f$ is discontinuous is at most countable.
</div>

<div class="Proof">
Wlog, $f$ is monotonically increasing. Let $E$ be the set of discontinuities of $f$.

With every point $x\in E$ we associate a rational number $r(x)$ such that $f(x-)< r(x) < f(x+)$. Since $x_1 < x_2$ implies $f(x_1 + )\le f(x_2-)$, we can see that $r(x_1)\ne r(x_2)$. But rational numbers are countable.
</div>

## Summary

In this lecture we discussed:

- limit of function and continuity;
- continuity interacting with compactness;
- two type of discontinuities;
- monotonic functions and their discontinuities.

# Differentiation (Lecture 10)



# Mean value theorem amongst other similar flavoured theorem (Lecture 11)

# Taylor Expansion (Lecture 12)


# exponentiation and logarithm (Lecture 13)


# sine and cosine (Lecture 14)


# Riemann integral I (Lecture 15)

# Riemann integral II (Lecture 16)

# integral and differentiation (Lecture 17)

# examples of Reimann integral (Lecture 18)

# improper Integral and series (Lecture 19)

# Application of integral (Lecture 20)


# References 


[^1]: see for example https://plato.stanford.edu/entries/conditionals/ and relevant pages for a start.

[^2]: If you don't believe this, try to find black snow and a not red grass.

[^3]: We need so called transitive sets (sets without $\in$-holes). Interested reader may find @jech2008axiom helpful.
